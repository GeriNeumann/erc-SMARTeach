\begin{refsection}

Incorporating cognitive robotics to skillfully manipulate a diverse array of objects—be they rigid, deformable, or articulated—in complex scenarios such as industrial assembly, logistics, or service robotics tasks like laundry handling, is a longstanding aspiration in the field of robotics. Achieving this breakthrough in SMARTeach could catalyze significant transformations across various industries, promoting efficient and sustainable practices.

A crucial aspect of this vision, particularly in economically advanced regions like the European Union, is addressing the pressing challenges posed by labor shortages, an aging workforce, and the shift of operations to lower-wage countries for maintaining competitive advantages. While robotics offers a promising solution, the intricate complexities and uncertainties of these tasks have limited its broader application. The key to overcoming these challenges lies in the concept of 'instructability'—the ability to imbue robots with the capacity to understand and learn from both existing knowledge sources and real-time human feedback.

Our project, SMARTeach, aims to revolutionize the field by making robot learning and teaching highly intuitive and interactive, and more importantly, instructable. This approach entails developing advanced bimanual robot-teleoperation platforms integrated with the latest augmented reality technologies, thereby enabling robots to not only comprehend complex manipulations but also to adapt and improve through continuously incorporating human feedback. The instructability in SMARTeach will be powered by utilizing extensive offline datasets, such as instruction manuals, to provide robots with foundational knowledge and context. This prior knowledge serves as a crucial starting point, allowing robots to quickly grasp the basics of a task. Complementing this, we will harness online human feedback, including evaluative feedback, demonstrations, and verbal instructions, to continually refine and adapt the robot's skills and understanding in real-time. By synergizing these two key elements—offline datasets and online feedback—our learning framework will orchestrate various paradigms such as imitation learning, reinforcement learning informed by human feedback, and meta-learning.
This iterative process is designed to steadily enhance robot capabilities, leading them to eventually outperform human tele-operators and achieve operational autonomy, significantly reducing the complexities and costs associated with deploying robotic technologies in novel industrial and service scenarios.

Inspired by real-world applications, our vision includes demonstrating the effectiveness of SMARTeach in high-impact use cases such as industrial assembly including mounting rubber-bands plugging in cables and screwing and bolting, assembly of complex structures guided by instruction manuals, and efficiently sorting laundry from a full basket. These scenarios not only exemplify the project's broad applicability but also highlight the transformative potential of instructability in robotics, paving the way for a new era of intelligent, adaptable, and efficient robotic solutions.

\subsection{Scientific Objectives and Challenges}
\textit{We aim to elevate robotic proficiency in diverse tasks, streamline data acquisition through intuitive tele-operation, continuously refine robot perception and skills using diverse human feedback, and optimize the deployment of independent robots within a fleet, all while reducing human intervention and maximizing operational efficiency.}
\vspace{-0.1cm}

\paragraph{O1: Streamline Intuitive Data Generation for Complex Tasks.} Advanced robotic manipulation relies heavily on a diverse range of annotated datasets. This includes demonstrations for skill development, visual data annotations for precise 6D scene reconstruction, and point-clouds for in-depth scene geometry. \newline
\textit{Challenges:} Acquiring this data is typically connected with high costs and has consistently posed challenges, often constraining broader industrial adoption of robotics. Instead of relying solely on large offline datasets that are hard to collect, methods and interfaces are needed that allow human operators to provide versatile feedback in real time.\newline
\textit{Scientific Objective:} Our objective is to streamline this data generation process, making it accessible even to those without a background in robotics. To achieve this, we will develop tele-operation interfaces grounded in human-centric design, setting a new benchmark in intuitive interaction. Furthermore, we are harnessing cutting-edge AR technologies to enhance the tele-presence experience for human operators. This strategy is geared towards enabling rapid and cost-effective data annotations and demonstrations. These advancements are pivotal for refining scene perception models, learning to predict action outcomes, and mastering intricate manipulation techniques.
\vspace{-0.1cm}
\paragraph{O2: Advance Learning and Representation of Robot Manipulation Skills.}  Our objective is to elevate the capabilities of robotics in handling diverse tasks, particularly those central to industrial applications like assembly, disassembly, sorting and serving as daily-life robotic aides. \newline
\textit{Challenges:} To achieve this, robots need to effortlessly manipulate environments abundant with unknown objects, clutter, and dynamic uncertainties, while ensuring consistent high performance. A core ambition is to achieve data-efficient mastery and refinement of object-centric manipulation skills. \newline
\textit{Scientific Objectives:} To realize this, we envision hierarchical manipulation skill representations that are scalable across numerous scene objects, adept at object selection, and optimized for fluid manipulation trajectories based on the target object's coordinates. This representation should be adaptable across different robot designs, be easily transferrable from virtual simulations to real-world applications, and flexibly adjust to various object geometries and demonstration speeds. Furthermore, we seek innovative learning algorithms to harness such representations, utilizing diverse input signals like demonstrations and reward feedback in reinforcement learning. \newline
\vspace{-0.5cm}
\paragraph{O3: Continuous Interactive Refinement of Perception, Prediction, and Skill Models.} Capitalizing on our advanced tele-operation platform, our goal is to develop learning algorithms that seamlessly integrate a range of human feedback mechanisms, encompassing corrections, pair-wise comparisons, numerical ratings, and verbal directives. \newline
\textit{Challenges:} A central challenge is the fusion of these diverse feedback modalities such as demonstrations, pairwise comparisons, corrections and verbal corrective instructions into a unified continual learning framework that can use the feedback to fine tune learned models and quickly adapt to different tasks. \newline
\textit{Scientific Objectives:} To address this, we plan to devise a comprehensive learning structure that adeptly merges varied learning strategies, from demonstration-based and model learning to reinforcement, interactive, and meta-learning. Through continuous refinement of perception, prediction, and skill execution based on feedback, our objective is to progressively enhance robotic capabilities. Over time, this will lead to diminished reliance on human supervision, envisioning a future where human interventions are either minimal or entirely unnecessary.

\vspace{-0.1cm}
\paragraph{O4: Seamless Integration and Fine-Tuning of Prior Foundational Knowledge and Instructions.} 
Large offline datasets and foundational models offer vital prior knowledge essential for complex object manipulation tasks. In scenarios such as assembly or cloth folding, a wealth of visual instructions available on the internet, like complex Lego structure manuals, can significantly expedite the robots' learning process. \newline
\textit{Challenges:} The primary challenge lies in the swift and computationally efficient fine-tuning of these foundational models for specific use cases. Common tasks addressed by these models are often simpler than the complex tasks we aim to tackle. Additionally, adapting foundational models to comprehend instruction manuals involves creating new datasets and parsing these manuals into actionable building steps. \newline
\textit{Scientific Objectives:} Our approach will enrich learning algorithms by integrating various information sources, including extensive offline datasets, foundational models, and simulated optimizations. The resulting models will be fine-tuned with online human feedback to enhance their adaptability and effectiveness. Furthermore, we will focus on integrating information from visual instructions for intricate manipulation tasks such as assembly and laundry folding. Through interactive teaching methods, robots will not only accomplish tasks based on diverse visual instructions but also transfer this acquired knowledge to new, unseen instructional contexts, thereby demonstrating advanced learning and adaptability capabilities.


%\paragraph{O4 Fleet Learning for Enhanced Productivity.} Our approach champions the deployment of a robotic fleet wherein each unit operates autonomously and independently, without inter-robot cooperation. This fleet, under the watchful supervision of a streamlined human team, aims to optimize cost efficiency and productivity as a human operator can supervise several robots at once. \newline
%\textit{Challenges:} The key challenge lies in pinpointing instances when a particular robot requires assistance and efficiently redirecting human expertise to these units. \newline
%\textit{Scientific Objectives:} Recognizing the inherent challenges of achieving full autonomy, especially considering sporadic operational anomalies, our ambition is to design collective error correction strategies and learning frameworks. To bolster this, we will incorporate uncertainty-aware learning methods for both skill acquisition and scene perception. This will empower robots in the fleet to autonomously identify and address discrepancies. As the system matures, we anticipate a significant reduction in human interventions, ensuring that any necessary human intervention is swift and effective, considerably enhancing the reliability and robustness of our approach. 

\subsection{State of the art and its \textit{limitations}}

\paragraph{Tele-operation and AR interfaces.} \textit{Data-collection interfaces for robot learning are often hard to use, are lacking 3D tele-presence or force feedback signals} (L1). Gamepads and Motion Controllers, despite their popularity, often come across as intricate tools, particularly for those unfamiliar with them \cite{Bushman_Asselmeier_Won_LaViers_2020}. In the domain of robot learning, crowd-sourcing data generation predominantly leans on virtual environments and gamepads, as highlighted by initiatives like CITE RoboTurk. However, gamepads inherently limit the depth of demonstrations. Similarly, screen-based visualizations compromise the operator's immersion, curtailing the intricacy of feasible tasks. While AR/VR technologies are becoming more commonplace, their utilization in the context of crowd-sourcing remains largely unexplored. Bimanual tele-operation systems, although emerging, are infrequent. Those that are available often do not offer a comprehensive 3D tele-presence or might omit essential features like force feedback \cite{Lipton_Fay_Rus_2018,DelPreto_Lipton_Sanneman_Fay_Fourie_Choi_Rus_2020,Jang_Niu_Collins_Weightman_Carrasco_Lennox_2021}.
\textit{A pronounced gap exists in embedding these teleoperation approaches within an interactive learning architecture} (L2). Seamless incorporation of operator inputs, ranging from new demonstrations to evaluative feedback, demands further exploration \cite{Hedlund_Johnson_Gombolay_2021,Moorman_Hedlund-Botti_Schrum_Natarajan_Gombolay_2023}. Lastly, while AR's efficacy in creating immersive experiences is recognized \cite{Mullen_Mosier_Chakrabarti_Chen_White_Losey_2021, Rosen_Whitney_Phillips_Chien_Tompkin_Konidaris_Tellex_2020}, advancing its role in clearly articulating robot decision-making remains a pivotal challenge, underlining the need to evolve AR beyond interaction towards establishing trust and understanding between robots and humans.
\vspace{-0.3cm}
\paragraph{Manipulation Skill Representations and Learning.} 
\textit{Current manipulation skill representations do not generalize well to new scenes, do not scale to a high number of objects to manipulate, and fail to capture smooth motions at different time scales} (L3). Such representations lean heavily on low-level joint commands, necessitating extensive demonstrations [CITE David]. Although methods like transformers and diffusion models [CITE transformers, diffusion] seek to embody the versatility of human demonstrations, hurdles persist. Using action chunking — predicting future actions rather than just one — can enhance performance. However, these predictions often result in trajectories that lack smoothness and don't capture the varied execution speeds seen in demonstrations. Movement primitives like DMPs, ProMPs, and ProDMPs offer these qualities and support non-linear replanning. Yet, their absence in object-centric, multi-modal architectures limits their generalization capabilities. 
\textit{RL methods for manipulation skills require too many samples to be applied on real robot systems} (L4). Crafting skill representations that adeptly explore trajectory spaces while observing environmental constraints is complex. Episodic exploration has given rise to black-box RL algorithms tailored for motion primitives, promoting temporally-correlated and smooth exploration policies, resulting in high-quality found policies. Although these black-box strategies are sample-inefficient, reuse of learned strategies from demonstrations, off-policy RL and model-based RL methods could improve their efficiency on real systems. Yet, the application of such methods for learning complex motion primitive-based manipulation skills is still unexplored. Optimization in simulation and fine tuning on the real system CITE a promising approach to make reinforcement learning more applicable on real systems, yet, obtaining simulations of a given task is often cumbersome CITE? and the quality of the policy is often decreased by the sim-2-real gap CITE.
\vspace{-0.3cm}
\paragraph{Interactive Robot Learning.} Interactive robot learning, often termed as learning from human feedback, represents a shift from the traditional paradigm of engineered reward representation. Instead, it harnesses feedback mechanisms like pairwise preferences and scorings to fine-tune policy behaviors. 
\textit{A key limitation of these methods is the use of rather uninformative feedback signal such as pairwise trajectory comparisons, resulting in a large number of human sequence queries to learn a task} (L5). The reliance on extensive preference feedback can be mitigated through the application of meta-learning techniques at the preference tier CITE and the synthesis of demonstrations and preferences within a unified framework. Yet, both approaches are still limited to rather simple tasks. Corrective feedback, although explored, has primarily been employed for simpler trajectory-following tasks. An innovative avenue emerging in this landscape is the application of large language models to condition reinforcement learning policies based on verbal directives. This presents a more intuitive way of guiding robots. However, the potential of language as a medium for real-time corrective feedback during the learning process remains largely untapped. \textit{Methods are needed that can unify versatile feedback types such as corrections via language or teleoperation and preference feedback into a single consistent framework} (L6).
\vspace{-0.3cm}
\paragraph{Model Learning for Manipulation.}

\paragraph{Foundation Models and Perception.} Foundation models for robotics offer transferable representations learned from large offline datasets [CITE RT1, RTX]. \textit{However, current foundation models can not be fine tuned easily for an unseen complex manipulation task and are therefore limited to rather simple standard manipulations learned from offline datasets} (L7). Methods are needed to interactively fine tune these learned representations during the learning process and scale them to scenes with many different objects of varying geometries and properties. While self-supervised CITE and predictive CITE representation learning approaches can learn good representations from high-dimensional sensory inputs for a single task, they do not generalize well to new tasks or scenes with many objects, clutter, occlusions, or distractions CITE. Here, creating object centric representations that include semantic/symbolic scene information which is readily available in simulation-based pre-training is a promising, yet unexplored avenue, to increase the generalization capabilities of current representation learning approaches. 
\textit{While deep learning-enhanced scene understanding algorithms such as 6D pose estimators CITE have achieved remarkable accuracy, their dependency on vast offline datasets and limited fine tuning capabilities has hindered their wide adoption in industries (L8)}. While category-agnostic algorithms mitigate this data demand, they often compromise prediction quality. Existing algorithms in this realm currently lack the flexibility of being conveniently corrected via on-the-spot feedback.


%Cost-effective Multi-Robot Learning. Despite huge progress in the last decade, current robot learning approaches often do not meet the high performance requirements across industries in terms of success rates and robustness (L9). Towards this end, training of a fleet of robots by human supervisors that are queried interactively to offer supplementary demonstrations when robots face failures is a promising strategy to increase the robustness of learning approaches at a moderate increase of costs. A pivotal aspect of this process is the efficient allocation of human experts to specific robot cells. A recent study by Hoque et al. [46] introduced heuristic query functions to discern the priority of queries emanating from robot cells. These heuristics, while useful, are tailored primarily for more straightforward tasks that have a limited solution scope. For more intricate multi-object manipulation tasks, there exists the flexibility to adjust the sequence of manipulations. This adaptability can be instrumental in better strategizing for manipulations that might pose challenges when executed autonomously. To harness this, there's a pressing need to devise skill learning techniques that are attuned to both uncertainty [CITE] and proficiency [CITE], thereby facilitating more precise and immediate querying of operator feedback. Yet, these methods have not been used yet for interactive learning in in a fleet of robots in an industrial context.

\subsection{Methodology and Organization}
SMARTeach is designed as holistic approach for intuitive and interactive robot teaching of complex manipulations. It aims at building new intuitive platforms for robot teaching as well as fundamental algorithmic developments for interactive learning of robot perception, scene modelling and learning with human feedback. All these algorithms will be considered in several complex use cases of complex manipulations of a large number of mixed objects which also includes understanding of visual and verbal instructions. 
\vspace{-0.3cm}
\paragraph{WP1: Enhancing Intuitiveness and Scalability in Robot Teaching.} WP1 is centered on creating intuitive robot teaching interfaces, leveraging teleoperation and AR technologies to allow even novices to perform complex manipulations and generate training data efficiently. 
\begin{enumerate}[noitemsep, topsep=0pt, partopsep=0pt, label=\alph*), leftmargin=0em, itemindent=1em, labelindent=1em, labelwidth=*]
\item \textit{\textbf{Bimanual Interactive Manipulation Platforms:}} We will develop bimanual teleoperation systems with AR capabilities for three robot platforms, including industrial robots with parallel jaw grippers, the advanced Shadow platform, and the humanoid ARMAR 7. These systems will focus on nuanced control and immersive interaction. 
\item \textit{\textbf{Crowd-Sourcing with Physical Simulators:}} Accurate physical simulators will be developed for each robot platform. Additionally, we will explore using cost-effective AR/VR gaming devices for remote operation, aiming to facilitate crowd-sourced demonstration collection via the internet. 
\item \textit{\textbf{AR Interface for Scene Comprehension and Correction:}} The AR interface will be enhanced for both visualization and user-driven correction of the robot’s environmental perception. This includes improving semantic segmentation and 6D pose estimation, and elucidating semantic object relations. 
\item \textit{\textbf{Shared Control and Corrective Algorithms:}} Algorithms will be developed to employ learned skill representations for aiding human operators via shared control, allowing for seamless intervention and correction of robot actions.
\end{enumerate}
\vspace{-0.3cm}
\paragraph{WP2: Interactive Learning for Enhanced Perception and Modelling.} In WP2, we focus on developing interactive foundation models that utilize large offline datasets and quickly adapt to online user annotations and corrections. These models will improve scene understanding (6D object poses, pixel-wise segmentations) and predictive geometric modeling of objects.
\begin{enumerate}[noitemsep, topsep=0pt, partopsep=0pt, label=\alph*), leftmargin=0em, itemindent=1em, labelindent=1em, labelwidth=*]
\item \textit{\textbf{Online Integration of User Annotations and Corrections:}} Building on existing foundation models [CITE] for scene understanding and 6D pose estimation, we will apply Bayesian meta-learning algorithms for rapid and robust fine-tuning, incorporating latent task descriptors for object models. 
\item \textit{\textbf{Learning Neuro-Symbolic Representations:}} We aim to develop generalizable representations that transfer well between robot architectures, using neural-symbolic architectures to learn and adjust object relations in real-time based on user input. 
\item \textit{\textbf{ Predictive Modeling of Object Dynamics:}} We'll enhance the learned representation by learning how rigid, deformable, and articulated objects change during manipulation using geometric modelling CITE and hence, improving geometric understanding of the scene. 
\item  \textit{\textbf{ Auto-generated Physics Simulators:}} The learned geometric and physical models will be integrated into physics simulations such as Isaac Sim. We will auto-generate meshes and physical properties from the sensor data and  annotations for easy import into simulation environments, facilitating the creation of tailored simulations and simulation-based optimization (see WP3) for a new use-case.
\end{enumerate}
\vspace{-0.3cm}
\paragraph{WP3: Scaling Up Manipulation Skill Learning.} 
WP3 focuses on advancing manipulation skill learning algorithms for scenarios involving numerous objects and complex interactions like insertion, screwing, and assembly. We aim to enhance data efficiency through cutting-edge off-policy and offline reinforcement learning (RL) algorithms, sim-to-real transfer, and model-based fine-tuning.
\begin{enumerate}[noitemsep, topsep=0pt, partopsep=0pt, label=\alph*), leftmargin=0em, itemindent=1em, labelindent=1em, labelwidth=*]
\item \textit{\textbf{Object-Centric Manipulation Skill Libraries:}} Building on our previous work with movement primitives, we'll develop new imitation learning algorithms for hierarchical skill representations that handle many objects. This object-centric architecture, integrating scene understanding from WP2, will efficiently select and manipulate objects based on their coordinates, scaling effectively to new object poses. 
\item \textit{\textbf{ Off-Policy and Offline RL for Skill Libraries:}} For this new skill architecture, we'll create novel off-policy and offline RL algorithms that unify imitation learning and reinforcement learning to refine policies learned from demonstration data, enhancing their applicability and efficiency.
\item \textit{\textbf{ Simulation-Based Pre-Tuning:}} Given the ease of policy learning in simulations, we will employ max-ent RL approaches [CITE] to make these policies more resilient to the sim-to-real gap. Efforts will also focus on adapting simulations to better reflect real-world conditions. 
\item \textit{\textbf{ Model-Based Fine-Tuning:}} Leveraging our prior work in model learning and object models from WP2, we'll develop architectures for multi-object manipulations. These models will be instrumental in optimizing and refining the skill libraries efficiently.
\end{enumerate}
\vspace{-0.3cm}
\paragraph{WP4: Learning from Human Feedback and Visual and Language Instructions}
WP4 is dedicated to developing methods for interactively fine-tuning object-centric manipulation skills, circumventing the need for extensively engineered reward functions. This process will leverage a variety of feedback types from human operators, including demonstrations, comparisons, haptic and verbal corrections, to develop adaptable reward representations. Additionally, robot task knowledge will be enriched by utilizing offline datasets and learning to interpret instruction manuals. 
\begin{enumerate}[noitemsep, topsep=0pt, partopsep=0pt, label=\alph*), leftmargin=0em, itemindent=1em, labelindent=1em, labelwidth=*]
\item \textit{\textbf{Generic Reward Representations from Multiple Feedback Sources:}} This subtask involves creating object-centric reward representations using demonstrations, pair-wise comparisons, and scorings. The focus here is to evaluate manipulation trajectories in accordance with object coordinate frames. Foundation models will initially act as a baseline for reward structures, which will then be fine-tuned through human feedback. 
\item \textit{\textbf{Incorporating Haptic and Verbal Corrections:}} We aim to provide users with intuitive correction mechanisms for robot behavior, which could be through haptic feedback using the shared control interface developed in WP1, or via verbal instructions (e.g., "you did not push hard enough"). Integration of large language models into the reward framework will enhance this interactive process. 
\item \textit{\textbf{Uncertainty-Aware Reward Representations:}} In this section, robots will be trained to identify errors and unmastered skills, leading to more directed queries to human operators. Error recognition models will be developed using human feedback, combined with Bayesian Deep Learning techniques to incorporate uncertainty awareness into these models. These uncertainty estimates will then be utilized to guide the query generation for human operators. 
\item \textit{\textbf{Learning from Visual and Language Instructions:}} This task encompasses the parsing of instruction manuals into discrete segments of visual and linguistic instructions. The parsed content will significantly influence the long-term planning hierarchy within the skill representation framework, with an emphasis on refining skill and object selection policies. Initially, interpreting these manuals using only foundational knowledge may prove challenging. To address this, we will employ our interactive teaching methods to enable the robot to understand and process a variety of instruction manuals. 
\end{enumerate}

\vspace{-0.3cm}
\paragraph{WP5: Use Cases and User Studies}
In WP5, we will focus on three high-impact use cases: searching and recovering objects, laundry sorting and folding, and the assembly of complex structures. Alongside, extensive user studies will evaluate the AR interfaces and interactive teaching algorithms.

\begin{enumerate}[noitemsep, topsep=0pt, partopsep=0pt, label=\alph*), leftmargin=0em, itemindent=1em, labelindent=1em, labelwidth=*]
    \item \textit{\textbf{Search and Recover Objects:}} Implemented with bimanual Franka Panda arms and an AR tele-operation system, this use case involves retrieving specific objects from a cluttered bin. To do so, the robot needs to get a good understanding of the cluttered scene and the involved objects. 
    \item \textit{\textbf{Laundry Sorting and Folding:}} Utilizing the humanoid robot Armar 7 and tele-operation via motion controllers and data-gloves, this task focuses on sorting and folding laundry, with visual folding instructions as guide.
    \item \textit{\textbf{Assembly of Complex Structures:}} This task will see the assembly of complex structures, such as Lego or Duplo models, following instruction manuals, using the shadow teleoperation platform for fine manipulation.
    \item \textit{\textbf{User Studies:}} Intensive studies will be conducted to assess the most effective control and AR interfaces and feedback types for these scenarios.
\end{enumerate}

%\paragraph{WP5: Use cases and User Studies.} In this WP, we will design 3 challenging use cases, i.e., industrial assembly, sorting and folding the laundry out of a full basket and assembly of complex "Lego-structures" following instruction manuals. Moreover, we will perform extensive user studies for the AR interfaces and interactive teaching algorithms. 
%\begin{enumerate}[noitemsep, topsep=0pt, partopsep=0pt, label=(\roman*), leftmargin=0em, itemindent=2em, labelindent=2em, labelwidth=*]
%\item \textit{\textbf{Industrial Assembly:} } This use-case will be implemented using bimanual Franka Panda arms and the tele-operation AR system illustrated in CITE. The robot has to plug in cables, screw, bolt and insert parts. The scenario will be developed in close collaboration with the start-up Artiminds CITE, who have access to real industrial use cases and where I already established a fruitful collaboration via joint paper submissions.
%\item \textit{\textbf{Search and Recover Objects:} }  This use-case will be implemented using bimanual Franka Panda arms and the tele-operation AR system illustrated in CITE. The robot needs to search and collect all objects of a given type from a clutterd bin containing a multitude of different objects. Objects need to be pushed away, grasped and removed from the bin. 
%\item \textit{\textbf{Laundry Sorting and Folding:} } This use-case will be implemented using the humanoid robot Armar 7 developed by the H2T team in Karlsruhe.  Tele-operation will be implemented via motion controllers and data-gloves to operate the fingers. The use-case will be developed in close collaboration with H2T, which is one of the leading robotics labs in Europe. The robot has to take clothes from the laundry basket, inspect them and fold them according the corresponding cloth category. In this task, we will also use visual instructions readily available in the internet of how to fold clothes as prior knowledge.  
%\item \textit{\textbf{Assembly of Complex Lego-Structures following Instruction Manuals:}} In this task, we require dexterous hands to manipulate small lego bricks such that the robot can build complex lego structures following instruction manuals. We will utilize the shadow teleoperation platform for doing so and teach the robot how to follow a diverse set of instruction manuals. In case the lego bricks are too fine grained, we will utilize larger bricks such as Lego Duplo. There exists over 300 instruction manuals for Lego Duplo and even more for standard Lego bricks. 
%\item \textit{\textbf{User Studies:}} We will perform intensive user studies to test what are the best control interfaces, AR interfaces and feedback types for these use cases.
%\end{enumerate}

\subsection{Impact}
\textbf{Scientific Impact:} SMARTeach marks a significant step forward in interactive learning and cognitive robotics. This project is not just about specific tasks; it's about setting a new standard in robotics capabilities across a spectrum of applications. By enhancing robot teaching interfaces and manipulation skills, SMARTeach will contribute to foundational advancements in AI, with implications for reinforcement learning, human-robot interaction, and control theory. The generic nature of our approach means that while we focus on examples like industrial assembly, logistics, and service robotics, the methodologies and technologies developed will have far-reaching applications in a multitude of experimental sciences and industries. \newline
\textbf{Industrial Impact:} SMARTeach will have a profound impact on the industrial sector, particularly in areas requiring precise and complex manipulation such as assembly lines. The project's advancements in intuitive robot teaching and scalable learning algorithms will enable more efficient, accurate, and flexible robotic systems, applicable to a wide range of industrial tasks beyond the specific examples of gearboxes or cables assembly. This will not only enhance productivity and quality but also address key challenges such as workforce shortages and workplace safety.
Beyond industrial applications, SMARTeach's technologies are poised to transform logistics and service sectors. For logistics, the ability to efficiently locate and package a variety of items will revolutionize warehouse operations. In service sectors, particularly in tasks like laundry handling, SMARTeach will pave the way for innovative robotic solutions, enhancing efficiency and user experience. \newline
\textbf{Societal Impact:} The overarching goal of SMARTeach is to provide technological solutions that address societal needs. By automating complex tasks, the project will alleviate labor shortages and contribute to safer work environments. The commitment to open-source, reproducible technology underlines SMARTeach’s dedication to accessible and equitable technological progress. In the long run, SMARTeach's contributions will not only streamline operations across various sectors but also support sustainable practices and enhance quality of life, demonstrating the project’s extensive societal benefits.

\subsection{Risk Assessment and Chances for Success}

SMARTeach, while high-risk, is counterbalanced by substantial risk mitigation measures. The project's resilience stems from its versatile approach, utilizing diverse feedback signals to ensure robustness even if certain feedback mechanisms are less effective. We have contingency plans for teleoperation interfaces, ready to switch to alternative technologies like motion controllers or haptic joysticks if necessary. Our project's strength is further bolstered by strong collaborations with leading industries such as Bosch, SAP, and innovative startups like Artiminds, as well as with prominent service robotics labs like H2T at the KIT. These partnerships are crucial in driving our use-case design, aligning our research with real-world applications. My personal long-term commitment to transforming reinforcement learning and robotics, particularly in fields like industrial assembly and household robotics, underscores the project's potential for impactful, sustained success.

\subsection{Resources and commitment of the PI}
I will commit 50\% of my time to SMARTeach. The person costs will go to 3 PhD students (4 years each) and 1 PostDoc position (3 years). In addition, I apply for funding for the Shadow Teleoperation System (500k€) that will be fully integrated in our interactive AR tele-operation pipeline and 3 low-cost Aloha platforms from Trossen robotics (24k€ each) to extend our approach to robot fleets and scale experimentation. 
% Add \subsection's as needed. It is possible to refer to a part of the
% B2 using \input{../part-B2/parts/file}; within this file, \ifbone can
% be used to determine if one is in the B1 or in the B2



% Add \nocite{} for any citation from the CV or EATR to add to the
% bibliography

\pagebreak
\defbibheading{subsection}[\bibname]{\subsection*{#1}}
\printbibliography[prenote=bolditalics,heading=subsection]

\end{refsection}
