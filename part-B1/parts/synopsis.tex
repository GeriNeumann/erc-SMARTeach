\begin{refsection}

%Incorporating cognitive robotics to skillfully manipulate a diverse array of objects—be they rigid, deformable, or articulated—in complex scenarios such as industrial assembly, logistics, or service robotics tasks like laundry handling, is a longstanding aspiration in the field of robotics. Achieving this breakthrough in SMARTeach could catalyze significant transformations across various industries, promoting efficient and sustainable practices.

%A crucial aspect of this vision, particularly in economically advanced regions like the European Union, is addressing the pressing challenges posed by labor shortages, an aging workforce, and the shift of operations to lower-wage countries for maintaining competitive advantages. While robotics offers a promising solution, the intricate complexities and uncertainties of these tasks have limited its broader application. The key to overcoming these challenges lies in the concept of 'instructability'—the ability to imbue robots with the capacity to understand and learn from both existing knowledge sources and real-time human feedback.

%Our project, SMARTeach, aims to revolutionize the field by making robot learning and teaching highly intuitive and interactive, and more importantly, instructable. This approach entails developing advanced bimanual robot-teleoperation platforms integrated with the latest augmented reality technologies, thereby enabling robots to not only comprehend complex manipulations but also to adapt and improve through continuously incorporating human feedback. The instructability in SMARTeach will be powered by utilizing extensive offline datasets, such as instruction manuals, to provide robots with foundational knowledge and context. This prior knowledge serves as a crucial starting point, allowing robots to quickly grasp the basics of a task. Complementing this, we will harness online human feedback, including evaluative feedback, demonstrations, and verbal instructions, to continually refine and adapt the robot's skills and understanding in real-time. By synergizing these two key elements—offline datasets and online feedback—our learning framework will orchestrate various paradigms such as imitation learning, reinforcement learning informed by human feedback, and meta-learning. This iterative process is designed to steadily enhance robot capabilities, leading them to eventually outperform human tele-operators and achieve operational autonomy, significantly reducing the complexities and costs associated with deploying robotic technologies in novel industrial and service scenarios.

%Inspired by real-world applications, our vision includes demonstrating the effectiveness of SMARTeach in high-impact use cases such as industrial assembly including mounting rubber-bands plugging in cables and screwing and bolting, assembly of complex structures guided by instruction manuals, and efficiently sorting laundry from a full basket. These scenarios not only exemplify the project's broad applicability but also highlight the transformative potential of instructability in robotics, paving the way for a new era of intelligent, adaptable, and efficient robotic solutions.

\subsection{Introduction}

The SMARTI$^3$ project targets pivotal advancements in robotics, aiming to fulfill three long-standing visions in the field: enabling robots to understand, reason about, and manipulate environments with a multitude of diverse objects;  equipping robots to handle objects with varying geometries and physical properties, including rigid, deformable, and articulated forms; and empowering robots to follow complex instruction manuals for task execution. 

While AI is predestined to realize these visions, its promises in the realm of robotics have yet to be fully realized due to three inherent challenges of using AI in robotics. Firstly, the process of data collection and labeling in robotics is typically burdensome, expensive, and highly task-specific. Secondly, AI-driven models often necessitate extensive training data for effective fine-tuning to specific tasks, a requirement that is frequently impractical in dynamic real-world scenarios. Thirdly, the application of AI to specialized robotic use cases demands substantial expertise, presenting significant adoption barriers, particularly for small and medium-sized enterprises (SMEs).

Confronting these challenges, SMARTeach proposes a comprehensive and integrated research approach. This strategy encompasses the development of intuitive methodologies for generating task-specific data, advanced techniques for fine-tuning foundational models, methods for leveraging human feedback to facilitate continual learning and adaptation, and the seamless integration of semantic and geometric knowledge from instruction manuals into robotic skills, perception, and prediction models.

This synopsis delineates the project's objectives, methodologies, and anticipated outcomes. We highlight the impact and practical application of SMARTeach through three illustrative use cases: sorting and disassembling diverse Lego pieces, folding paper into origami guided by visual instructions, and assembling intricate Lego structures from an unsorted collection using instruction manuals. These use cases are thoughtfully selected not only to demonstrate the feasibility of our approach but also to lay the groundwork for future innovations in intelligent, adaptable, and user-friendly robotic solutions.



\subsection{Scientific Objectives and Challenges}
\textit{We aim to streamline data acquisition through intuitive teleoperation, enhance robotic proficiency by leveraging foundational representations as prior knowledge, improve robot skill learning through hierarchical skill representations, and continually refine robot perception and skills using diverse human feedback and offline instructions.}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\vspace{-0.1cm}
\paragraph{O1: Streamline Human and Synthetic Data Generation.} Advanced robotic manipulation relies heavily on a diverse range of annotated datasets. This includes demonstrations for skill development, visual data annotations for precise 6D scene reconstruction, and point-clouds for in-depth scene geometry. \newline
\textit{Challenges:} Acquiring this data is typically connected with high costs and has consistently posed challenges, often constraining broader industrial adoption of robotics. Instead of relying solely on large offline datasets that are hard to collect, methods and interfaces are needed that allow human operators to provide versatile feedback in real-time.\newline
\textit{Scientific Objective:} Our objective is to streamline this data generation process, making it accessible even to those without a background in robotics. To achieve this, we will develop tele-operation interfaces grounded in human-centric design, setting a new benchmark in intuitive interaction. Furthermore, we are harnessing cutting-edge AR technologies to enhance the telepresence experience for human operators. This strategy is geared towards enabling rapid and cost-effective data annotations and demonstrations. These advancements are pivotal for refining scene perception models, learning to predict action outcomes, and mastering intricate manipulation techniques.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\vspace{-0.1cm}
\paragraph{O2: Few-Shot Adaptation of Foundational Representations.} 
Foundational models provide rich representations that serve as prior knowledge for the acquisition of robot skills, perception, and world models.
\newline
\textit{Challenges:} 
% The current foundational models in robotics and robot perception exhibit impressive performance across various datasets but face challenges in fine-tuning for specific tasks due to the limited availability of task-specific data.
Current approaches to fine-tuning foundation models often rely on extensive data, address single tasks in isolation without accounting for uncertainty in data and predictions, resulting in limited generalizability to varying scenarios, and face challenges in learning object interaction models from raw sensory data.
\newline
\textit{Scientific Objective:}  We aim to develop sophisticated few-shot learning algorithms that enhance pre-trained foundational models, focusing on robot skills, perception, and world models predicting object dynamics. These models, characterized by their awareness of uncertainty, demonstrate robust generalization to novel tasks.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\vspace{-0.1cm}
\paragraph{O3: Scaling-Up Robot Skill Learning.} 
Enhancing robot skill learning is crucial for elevating the capabilities of robotics, especially in performing diverse tasks essential to industrial applications such as assembly, disassembly, sorting, and serving as daily-life robotic aides.
\newline
\textit{Challenges:} 
Current methods face challenges in adapting to new scenes with multiple objects, translating learned simulation behaviors to real-world systems due to dynamic discrepancies, and dealing with high-dimensional manipulators given the complexity of state representation and action space.
\newline
\textit{Scientific Objective:} 
Our goal is to design algorithms that: i) leverage hierarchical skill representations, enabling manipulation of a large number of objects while adapting to object geometry and physical properties for generalizing to new scenes, ii) are robust to simulation errors and therefore require a small amount of fine-tuning steps on the real system, and iii) address high-dimensional challenges by learning efficient representations on a low-dimensional manifold.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\vspace{-0.1cm}
\paragraph{O4: Learning from Interactive Human Feedback and Offline Instructions.} Efficient learning of robotic skills requires the integration of diverse human feedback mechanisms, including demonstrations, corrections per tele-operation and verbal corrections as well as pair-wise comparisons. Additionally, robots should be capable of learning from both verbal commands and visual instructions extracted from manuals.\newline
\textit{Challenges:} 
Existing methods for learning from human feedback encounter challenges such as requiring numerous feedback queries or being unable to integrate diverse feedback such as kinesthetic and verbal corrections into a unified reward representation. Moreover, current methods primarily focus on verbal offline instructions from large language models and have not been adapted to incorporate more informative multi-step visual instructions found in instruction manuals.
\newline
\textit{Scientific Objective:} Our objective is to develop generic reward representations capable of incorporating various feedback modalities. Specifically, these representations utilize foundation models to encode visual instruction information and then undergo fine-tuning through human feedback, including haptic corrections.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% \vspace{-0.1cm}
% \paragraph{O2: Advance Learning and Representation of Robot Manipulation Skills.}  Our objective is to elevate the capabilities of robotics in handling diverse tasks, particularly those central to industrial applications like assembly, disassembly, sorting and serving as daily-life robotic aides. \newline
% \textit{Challenges:} To achieve this, robots need to effortlessly manipulate environments abundant with unknown objects, clutter, and dynamic uncertainties, while ensuring consistent high performance. A core ambition is to achieve data-efficient mastery and refinement of object-centric manipulation skills. \newline
% \textit{Scientific Objectives:} To realize this, we envision hierarchical manipulation skill representations that are scalable across numerous scene objects, adept at object selection, and optimized for fluid manipulation trajectories based on the target object's coordinates. This representation should be adaptable across different robot designs, be easily transferrable from virtual simulations to real-world applications, and flexibly adjust to various object geometries and demonstration speeds. Furthermore, we seek innovative learning algorithms to harness such representations, utilizing diverse input signals like demonstrations and reward feedback in reinforcement learning. \newline
% \vspace{-0.5cm}
% \paragraph{O3: Continuous Interactive Refinement of Perception, Prediction, and Skill Models.} Capitalizing on our advanced tele-operation platform, our goal is to develop learning algorithms that seamlessly integrate a range of human feedback mechanisms, encompassing corrections, pair-wise comparisons, numerical ratings, and verbal directives. \newline
% \textit{Challenges:} A central challenge is the fusion of these diverse feedback modalities such as demonstrations, pairwise comparisons, corrections and verbal corrective instructions into a unified continual learning framework that can use the feedback to fine tune learned models and quickly adapt to different tasks. \newline
% \textit{Scientific Objectives:} To address this, we plan to devise a comprehensive learning structure that adeptly merges varied learning strategies, from demonstration-based and model learning to reinforcement, interactive, and meta-learning. Through continuous refinement of perception, prediction, and skill execution based on feedback, our objective is to progressively enhance robotic capabilities. Over time, this will lead to diminished reliance on human supervision, envisioning a future where human interventions are either minimal or entirely unnecessary.

% \vspace{-0.1cm}
% \paragraph{O4: Seamless Integration and Fine-Tuning of Prior Foundational Knowledge and Instructions.} 
% Large offline datasets and foundational models offer vital prior knowledge essential for complex object manipulation tasks. In scenarios such as assembly or cloth folding, a wealth of visual instructions available on the internet, like complex Lego structure manuals, can significantly expedite the robots' learning process. \newline
% \textit{Challenges:} The primary challenge lies in the swift and computationally efficient fine-tuning of these foundational models for specific use cases. Common tasks addressed by these models are often simpler than the complex tasks we aim to tackle. Additionally, adapting foundational models to comprehend instruction manuals involves creating new datasets and parsing these manuals into actionable building steps. \newline
% \textit{Scientific Objectives:} Our approach will enrich learning algorithms by integrating various information sources, including extensive offline datasets, foundational models, and simulated optimizations. The resulting models will be fine-tuned with online human feedback to enhance their adaptability and effectiveness. Furthermore, we will focus on integrating information from visual instructions for intricate manipulation tasks such as assembly and laundry folding. Through interactive teaching methods, robots will not only accomplish tasks based on diverse visual instructions but also transfer this acquired knowledge to new, unseen instructional contexts, thereby demonstrating advanced learning and adaptability capabilities.


%\paragraph{O4 Fleet Learning for Enhanced Productivity.} Our approach champions the deployment of a robotic fleet wherein each unit operates autonomously and independently, without inter-robot cooperation. This fleet, under the watchful supervision of a streamlined human team, aims to optimize cost efficiency and productivity as a human operator can supervise several robots at once. \newline
%\textit{Challenges:} The key challenge lies in pinpointing instances when a particular robot requires assistance and efficiently redirecting human expertise to these units. \newline
%\textit{Scientific Objectives:} Recognizing the inherent challenges of achieving full autonomy, especially considering sporadic operational anomalies, our ambition is to design collective error correction strategies and learning frameworks. To bolster this, we will incorporate uncertainty-aware learning methods for both skill acquisition and scene perception. This will empower robots in the fleet to autonomously identify and address discrepancies. As the system matures, we anticipate a significant reduction in human interventions, ensuring that any necessary human intervention is swift and effective, considerably enhancing the reliability and robustness of our approach. 

\subsection{State of the art and its \textit{limitations}}

\paragraph{Streamline Human and Synthetic Data Generation.}
% \textit{Data-collection interfaces for robot learning are often hard to use, are lacking 3D tele-presence or force feedback signals} (L1). Gamepads and Motion Controllers, despite their popularity, often come across as intricate tools, particularly for those unfamiliar with them \cite{Bushman_Asselmeier_Won_LaViers_2020}. In the domain of robot learning, crowd-sourcing data generation predominantly leans on virtual environments and gamepads, as highlighted by initiatives like CITE RoboTurk. However, gamepads inherently limit the depth of demonstrations. Similarly, screen-based visualizations compromise the operator's immersion, curtailing the intricacy of feasible tasks. While AR/VR technologies are becoming more commonplace, their utilization in the context of crowd-sourcing remains largely unexplored. Bimanual tele-operation systems, although emerging, are infrequent. Those that are available often do not offer a comprehensive 3D tele-presence or might omit essential features like force feedback \cite{Lipton_Fay_Rus_2018,DelPreto_Lipton_Sanneman_Fay_Fourie_Choi_Rus_2020,Jang_Niu_Collins_Weightman_Carrasco_Lennox_2021}.
% \textit{A pronounced gap exists in embedding these teleoperation approaches within an interactive learning architecture}(L2). Seamless incorporation of operator inputs, ranging from new demonstrations to evaluative feedback, demands further exploration \cite{Hedlund_Johnson_Gombolay_2021,Moorman_Hedlund-Botti_Schrum_Natarajan_Gombolay_2023}. Lastly, while AR's efficacy in creating immersive experiences is recognized \cite{Mullen_Mosier_Chakrabarti_Chen_White_Losey_2021, Rosen_Whitney_Phillips_Chien_Tompkin_Konidaris_Tellex_2020}, advancing its role in clearly articulating robot decision-making remains a pivotal challenge, underlining the need to evolve AR beyond interaction towards establishing trust and understanding between robots and humans.

Building environments for the data generation process is crucial, yet it requires significant effort to design a unified environment that exhibits similar behaviors in both real and simulated worlds. Traditional approaches often tackle this challenge separately. While kinesthetic teaching is an off-the-shell technique for collecting demonstrations from humans \cite{Wrede_Emmerich_Grünberg_Nordmann_Swadzba_Steil_2013, Ravichandar_Polydoros_Chernova_Billard_2020, Sukkar_Moreno_Vidal_Calleja_Deuse_2023}, an ideal corresponding digital twin should be capable of simulating various types of dynamics, e.g, rigid, fluid and continuum and modeling diverse geometries and materials \cite{mittal2023orbit}.  Tele-operations under Augmented Reality (AR) environment emerges as a promising methodology to bridge this gap \cite{Mullen_Mosier_Chakrabarti_Chen_White_Losey_2021, Rosen_Whitney_Phillips_Chien_Tompkin_Konidaris_Tellex_2020}. It enhances users' immersive experiences by providing a tangible interface within interactable bounding boxes in the AR environment, allowing direct interaction with these visualizations. However, for more complex scenarios like data generation with bimanual tele-operation, AR remains in its early stages due to limitations in intuitive control, 3D tele-presence and the absence of force feedback.

\textbf{Own prior work}: Our group has recently introduced kinesthetic tele-operation. Specifically, we employ a leader-follower system where the leader is controlled through kinesthetic tele-operation in an AR environment, and the follower robot moves accordingly \cite{Sing_teleop}. Our study highlighted the effectiveness of this interface, achieving an impressive 97\% success rate across various tasks and participants \cite{jiang2023user}. This kinesthetic interface was further utilized in \cite{David2024} for data collection and benchmarking in the context of contemporary imitation learning algorithms, demonstrating its efficacy and potential for broader application in robot learning.

% Kinesthetic teaching is widely recognized as a standard method for collecting data from humans in robot learning \cite{jiang2023user, Wrede_Emmerich_Grünberg_Nordmann_Swadzba_Steil_2013, Ravichandar_Polydoros_Chernova_Billard_2020, Sukkar_Moreno_Vidal_Calleja_Deuse_2023}. However, due to sensory inaccuracy, model errors, and uncertainty in real physical world, giving a precise control command to move robots to desired targets during manipulation is challenging. Tele-operation emerges as a promising alternative, allowing users to directly provide control commands. It is implemented using devices like gamepads and motion controllers, as seen in crowd-sourced robot learning projects \cite{mandlekar2018roboturk}. 
% Yet, these methods inherently limit the complexity of demonstrations due to their lack of immersive 3D tele-presence capabilities. Recent research has begun to utilize augmented reality (AR) glasses to provide immersive experiences \cite{Mullen_Mosier_Chakrabarti_Chen_White_Losey_2021, Rosen_Whitney_Phillips_Chien_Tompkin_Konidaris_Tellex_2020}. For example, AR can be used to enhance 6D pose estimation of objects in real-time \cite{sahin2020review, firintepe2021ir}. This setup provides users a tangible interface within interactable bounding boxes in the AR environment and allows them to interact directly with these visualizations including translation, rotation, and scaling. However, this interaction approach is not precise and relies on the accuracy and stability of the hand tracking system. Our recent work introduced teleoperation by kinesthetic teaching as a new control interface in \cite{jiang2023user}. In a study where users had to control a virtualized robot, we showed that this interface is more intuitive to use than other control methods, resulting in higher task completion rates.
% However, for more complex scenarios like data generation with bimanual tele-operation, AR remains in its early stages due to limitations in intuitive control, 3D tele-presence and the absence of force feedback.

% Virtualization is a crucial tool in robot learning for simplified generation of demonstration data  \cite{jiang2023user, mandlekar2018roboturk} and applying reinforcement learning algorithms \cite{Das_Bechtle_Davchev_Jayaraman_Rai_Meier_2021, serhan2022push, Zenkri2022} to robotics tasks. Omniverse IsaacSim \cite{mittal2023orbit} emerges as a unified simulation that is particularly well suited for this. It can simulate different types of dynamics (e.g, rigid, fluid and continuum) and generates various geometries and materials. These features essentially help accelerate the scene generation process. Enhancing task diversity is also important in robot learning \cite{fang2023active}, and recent research has employed foundation models to allow users to generate scenes more interactively through verbal instructions \cite{wang2023gen}. Yet, the underlying simulated dynamics are highly simplified. For more specific dynamics, tailored simulation tools \cite{abaqus} are needed, which are often computationally demanding and require a lot of technical knowledge.
% Data-driven simulators have emerged as a promising approach that attempts to resolve this issue \cite{pfaff2021learning, sundaresan2022diffcloud, kandukuri2022physical, linkerhagnerFSM23}. In this case, the simulator is usually modelled by a deep neural network and learned directly from simulation data \cite{pfaff2021learning,linkerhagnerFSM23} or raw-sensory data \cite{sundaresan2022diffcloud,kandukuri2022physical} to approximate the dynamics of real environments.  Yet, creating a simulation for a specific task is still demanding effort as we have to create object models, including their geometry and physical properties.
% \todo{- How to automatically import objects into the virtual twin just by showing the objects to a camera and manipulating them with the robot?}
% \todo{\st{- How to auto-generate simulations using verbal instructions? https://arxiv.org/pdf/2310.01361.pdf. Maybe also have a look in the references of this paper}}
% \todo{- How to auto-generate object shapes from language: https://github.com/openai/shap-e}


\paragraph{Few-Shot Adaptation of Foundational Representations.} 
% \textit{Current manipulation skill representations do not generalize well to new scenes, do not scale to a high number of objects to manipulate, and fail to capture smooth motions at different time scales} (L3). Such representations lean heavily on low-level joint commands, necessitating extensive demonstrations \cite{David2024}. Although methods like transformers and diffusion models \cite{vaswani2017attention, ho2020denoising} seek to embody the versatility of human demonstrations, hurdles persist. Using action chunking — predicting future actions rather than just one — can enhance performance. However, these predictions often result in trajectories that lack smoothness and don't capture the varied execution speeds seen in demonstrations. Movement primitives like DMPs, ProMPs, and ProDMPs offer these qualities and support non-linear replanning. Yet, their absence in object-centric, multi-modal architectures limits their generalization capabilities. 
% \textit{RL methods for manipulation skills require too many samples to be applied on real robot systems} (L4). Crafting skill representations that adeptly explore trajectory spaces while observing environmental constraints is complex. Episodic exploration has given rise to black-box RL algorithms tailored for motion primitives, promoting temporally-correlated and smooth exploration policies, resulting in high-quality found policies. Although these black-box strategies are sample-inefficient, reuse of learned strategies from demonstrations, off-policy RL and model-based RL methods could improve their efficiency on real systems. Yet, the application of such methods for learning complex motion primitive-based manipulation skills is still unexplored. Optimization in simulation and fine tuning on the real system \cite{tobin2017domain, song2020learning} a promising approach to make reinforcement learning more applicable on real systems, yet, obtaining simulations of a given task is often cumbersome \cite{fang2023active, wang2023gen}? and the quality of the policy is often decreased by the sim-2-real gap \cite{zhao2020sim2real}.

Foundation models \cite{bommasani2022opportunities} are a popular choice for perception~\cite{Radosavovic2022mvp, karamcheti2023voltron, SAM} and behavior learning~\cite{rt12022arxiv, rt22023arxiv, rtx2023arxiv}. 
These models, while powerful, require task-specific fine-tuning~\cite{hu2022lora}.
This is challenging given the extreme data scarcity in robotic tasks such as 6D pose estimation~\cite{FFB6D,DenseFusion,pvn3d}, instance-based scene segmentation\cite{SAM,DINOv2,UOC,He_2017_ICCV}, or behavior preiction \cite{rt12022arxiv,rtx2023arxiv,rt22023arxiv} as every new scene, object, or manipulation can constitute a new task.
While fine-tuning on such tasks can be done with minimal data using meta-learning~\cite{finn2017model, garnelo12018np}, these methods have not yet been successfully combined with foundation models.
For applications such as e.g. robotic origami folding~\cite{NamikiY15, ThaiSH18}, learned object dynamics~\cite{pfaff2021learning} and time-series prediction~\cite{gu2021efficiently, smith2022simplified} models will be crucial for accurate planning and model-based reinforcement learning.
However, fine-tuning large foundation models with online data is still a challenge~\cite{xian2021hyperdynamics}.

\textbf{Own Prior Work}: Our lab explored meta-learning~\cite{Gao_2022_CVPR,volpp2021bayesian} and Bayesian Deep Learning~\cite{wilson2020bayesian, seligmann2023beyond} to allow for sample-efficient fine-tuning with models that are appropriate for the safety concerns arising in robotics~\cite{volpp2021bayesian, Volpp23}.
Our research focuses on the prediction of object deformations from sensory information~\cite{linkerhagnerFSM23} and long-term time-series models~\cite{shaj2020acrkn, shaj2022hiprssm, shaj2023mts3}, especially with respect to uncertainty in the models observations~\cite{becker19RKN, becker2022uncertainty}.


\paragraph{Scaling-Up Robot Skill Learning.}
Imitation learning strategies represent a shift from the traditional paradigm of engineered reward representation toward human feedback.
Existing strategies use diffusion models~\cite{shafiullah2022behavior, chi2023diffusion, pearce2023imitating}, vision-based representations~\cite{zeng2018learning, David2024, zeng2020tossingbot, serhan2022push} and scene factorization~\cite{zhu2023viola} to represent complex multi-object scenes.
Yet, these methods rely on low-level joint commands, require extensive demonstrations and are often limited to simpler tasks, focusing on simpler grippers or robotic hands with limited degrees of freedom \cite{chen2023development, saloutos2023towards, cairnes2023overview}.
\textit{A key limitation of these methods is the inefficient use of human feedback, resulting in a large number of human sequence queries to learn a task.} 
To more effectively use this feedback and generate trajectories that can be executed on a real robot, object-centric representations~\cite{Jianfeng2023KVIL, mandlekar2023mimicgen, freymuth2022vigor} and motion primitives~\cite{schaal2006dynamic, paraschos2013probabilistic, li2023prodmp} can be used.
Another direction focuses on fine-tuning imitation learning methods using RL~\cite{Zhu-RSS-18,pmlr-v155-julian21a,Rajeswaran2018Dexterous, ball2023efficient}, which is limited to simple policy representations or requires explicit reward-labelled demonstrations.
\todo{add something with model-based RL and sim2real/real2sim from the commented-out parts below. @Balazs?}

% Interactive robot learning, often termed as learning from human feedback, represents a shift from the traditional paradigm of engineered reward representation. Instead, it harnesses feedback mechanisms like pairwise preferences and scorings to fine-tune policy behaviors. 
% \textit{A key limitation of these methods is the use of rather uninformative feedback signal such as pairwise trajectory comparisons, resulting in a large number of human sequence queries to learn a task} (L5). The reliance on extensive preference feedback can be mitigated through the application of meta-learning techniques at the preference tier CITE and the synthesis of demonstrations and preferences within a unified framework. Yet, both approaches are still limited to rather simple tasks. Corrective feedback, although explored, has primarily been employed for simpler trajectory-following tasks. An innovative avenue emerging in this landscape is the application of large language models to condition reinforcement learning policies based on verbal directives. This presents a more intuitive way of guiding robots. However, the potential of language as a medium for real-time corrective feedback during the learning process remains largely untapped. \textit{Methods are needed that can unify versatile feedback types such as corrections via language or teleoperation and preference feedback into a single consistent framework} (L6).

\textbf{Own Prior Work}:
Our lab explores different scene representations~\cite{David2024, serhan2022push, freymuth2022vigor} for data-efficient imitation learning.
Additionally, we have experience in combining motion primitives~\cite{paraschos2013probabilistic,li2023prodmp} with RL on a trajectory level~\cite{celik2022specializing,otto2023deep} to allow for smooth policy exploration and execution on a real robot.
We combine this knowledge with expertise in model-based RL~\cite{becker19RKN, shaj2020acrkn} and adapting simulations using sensory data~\cite{linkerhagnerFSM23}.

%% What we have in B2 is this:

% Imitation learning strategies primarily use low-level joint commands and extensive demonstrations, but recent models like transformers and diffusion models, while aiming to capture diverse human demonstrations and predict long-term behavior, struggle with multiple objects in a scene; some methods employ vision-based approaches for object management but are restricted to simpler tasks, whereas others suggest modular frameworks with limited adaptability. In contrast, several studies focus on object-centric solutions that offer better generalization to novel object configurations.

% Many imitation learning strategies rely on low-level joint commands and extensive demonstrations \cite{David2024}. While recent approaches like transformers and diffusion models aim to encapsulate human demonstration diversity and action chunking to predict long-term behavior \cite{shafiullah2022behavior, chi2023diffusion, pearce2023imitating}, they face challenges dealing with many objects in a scene \cite{David2024}. Some methods, like \cite{zeng2018learning, David2024, zeng2020tossingbot, serhan2022push}, use vision-based representations for managing multiple objects but are generally limited to simpler 2D table-top tasks. Alternatively, studies like \cite{zhu2023viola} propose factorizing scene representations and operational skills in multi-object environments, resulting in a modular manipulation framework with limited online adaptation. 
% In contrast to robot-centric representation, many studies investigate on object-centric solutions \cite{Jianfeng2023KVIL, mandlekar2023mimicgen} that generalize better to unseen object configurations. 
% Motion primitives \cite{schaal2006dynamic, paraschos2013probabilistic, li2023prodmp} are crucial for smooth trajectory representation, enabling trajectory sampling, continuous replanning \cite{otto2023mp3}, and versatile skills learning \cite{celik2022specializing}. Work in \cite{Jianfeng2023KVIL} combines object-centric approaches with via-point movement primitives \cite{zhou2019learning}, learning manipulation trajectories of single objects as end-effector tools. While this can generalize to different tool geometries, there is no extension to scenes with a large number of objects. 

% Fine-tuning policies derived from imitation learning using reinforcement learning (RL) is an area of active research \cite{Zhu-RSS-18,pmlr-v155-julian21a,Rajeswaran2018Dexterous}. However, these methods typically resort to simplistic policy representations, such as Gaussian policies, which fall short in capturing the complexities of the advanced manipulation skills targeted in this project. Moreover, the standard Gaussian exploration noise used in RL, which is problematic for real-robot execution as well as effectively supporting the human operator in providing corrections for refining skill executions.
% One potential solution to smooth exploration is the use of episodic RL algorithms \cite{otto2023deep} that explore directly in trajectory space using motion primitives \cite{paraschos2013probabilistic,li2023prodmp}, thereby easing human operator corrections. Nevertheless, these algorithms have yet to be successfully integrated into hierarchical policy frameworks and suffer from sample inefficiency due to their on-policy nature.
% Alternatively, more data-efficient approaches like off-policy methods 
% \cite{lillicrap2015continuous, haarnoja2018soft, fujimoto2018addressing}
% , offline RL methods 
% \cite{kumar2020conservative,kostrikov2021offline},
% and model-based RL strategies (\cite{becker19RKN, shaj2020acrkn}), present interesting possibilities. Yet, these methods struggle to produce smooth motion trajectories as they have not been combined with motion primitive-based policy representations. 
% Here, shared control strategies have been used to add user-guidance to off-policy RL such that the policy can explicitly satisfy task constraints and improves the learning efficiency. 
% Some works such as \cite{ball2023efficient}, fine-tune imitation learning policies using online RL. However, their reliance of explicit reward-labelled demonstrations complicates their further extensions to complex manipulation tasks that require hierarchical skills. 

% Previous research focused on using hierarchical RL for learning complex manipulation skills. In \cite{gupta2019relay} hierarchical RL was used to refine multi-state, long-horizon robotic tasks in simulations based on unstructured and imperfect demonstrations, whereas in \cite{huang2023reparameterized} an objective for training hierarchical policies using a learned world model was presented. However, the absence of action smoothness in these methods could introduce unforeseen risks when directly applied to real-world systems. 

% Several strategies exist to mitigate the sim-to-real gap. Domain randomization is the most widely used technique \cite{tobin2017domain}. It generates a wide range of diverse dynamics during learning, thereby enhancing the robustness of the resulting policy. One can also employ maximum entropy (max-ent) strategies in the optimization process to further prevent the overfitting problem \cite{eysenbach2022maximum, tiboni2023domain}. Another promising approach is to continuously adapt the simulation models \cite{clavera2018learning, linkerhagnerFSM23} using real sensory data
% . In cases where the simulations are learned models, one can fine-tune them to better match real-world conditions \cite{song2020learning}. This strategy can be applied to highly complex tasks; for example, Robocook \cite{shi2023robocook} fine-tuned a learned Graph  Neural Networks dynamics model to successfully make dumplings in the real world. Yet, it remains unclear what to do if a significant sim-2-real gap remains, which is foreseeable for more complex multi-step tasks.

% Current research in robotic manipulation primarily focuses on simpler grippers or robotic hands with limited degrees of freedom \cite{chen2023development, saloutos2023towards, cairnes2023overview}, which are ill-suited for intricate tasks like tool usage. A better fit are high dimensional manipulators, such as the Shadow hand considered in this project, as they provide more flexibility while allowing for fine manipulation~\cite{negrello2020hands}. Yet, existing techniques struggle with the high number of actuated degrees of freedom, limiting their applicability to grasps and simple manipulations ~\cite{liu2022herd,Li2022Survey,kadalagere2023Review}. Complex in-hand manipulations have been considered using RL algorithms in simulation which also has been transferred to the real system \todo{CITE openAI} \cite{shadowhand}. However, the learned policy is only suitable for a single type of manipulation of a single object shape. 
% \cite{Rajeswaran2018Dexterous} used imitation learning and RL-based fine tuning to learn policies in simulation for dexterous hands. Additionally, \cite{qin2022from, qin2021dexmv} proposed a teleoperation system to map human hands to multiple robotic hands. Demonstrations collected by this system have been proven effective in improving RL policies in both simulation and real robot experiments.
% Current state-of-the-art solutions, including various motion representation models, provide a foundational understanding but lack the necessary complexity to handle detailed tactile, geometric, and visual data integration required for dexterous robotic hands~\cite{navarro2023visuo}. While dexterous hands define a very high dimensional control problem, it is well known that most human manipulation strategies live on lower-dimensional sub-spaces allowing, for instance, to learn models that map high dimensional grasp configurations into a lower dimensional grasp space \cite{starke2018synergy, starke2021tempsynergy, rivera2021synergy}.


\paragraph{Learning from Interactive Human Feedback and Offline Instructions.}
Defining an appropriate reward for an RL task often requires significant expertise and task-specific knowledge, plus extensive fine-tuning. An alternative is learning reward functions from diverse source of human feedback like demonstrations \cite{Takayuki2018ImitationLearning}, pairwise comparisons \cite{wirth2016PrefRL,ChristianoNIPS2017}, scorings \cite{Garrett2018DeepTamer}, corrective feedback \cite{Losey2022PCorrections}, or verbal instructions \cite{Pratyusha2020CorrectingNLF}. 
Current methods face a significant limitation: learned reward representations are task-specific, limiting transferability to similar tasks and requiring substantial feedback for effective learning \cite{brown2019drex, Biyik2018BatchPref, Biyik2022Learning}.
The rise of large language models has generated interest in using language instructions for robot learning \cite{saycan2022arxiv,Pratyusha2020CorrectingNLF,nair2021learning}, but these approaches are still in their infancy.
Learning tasks from visual instructions in manuals is only rarely seen so far in the literature, limited to video games \cite{wu2023read}, where only text-based manuals are considered.
Methods exist for translating 2D LEGO manuals into machine-compatible instructions with 3D poses and positions\cite{wang2022translating}, but have never been integrated with reward feedback from an RL environment.

\textbf{Own Prior Work}:
Our lab explores combining several types of feedback into a unified framework, using the popular adversarial imitation learning paradigm~\cite{taranovic2023ailp}.
In future work, we will extend this framework to include further feedback modalities, such as corrections.
Matching expert demonstrations in terms of concise geometric descriptors rather than in terms of observations also increases generalization to new tasks~\cite{freymuth2022vigor, Volpp23}.
%\cite{volpp23}


% In the context of verbal-based policy, fine-tuning pre-trained vision-language foundation models is also a popular approach to acquiring generalized policies \cite{ge2023policy}. However, for tasks that require more precision, applying these strategies often involves extensive fine-tuning of the policies on the real system. \todo{This rather fits foundational models or instructions - move to different section}

%Cost-effective Multi-Robot Learning. Despite huge progress in the last decade, current robot learning approaches often do not meet the high performance requirements across industries in terms of success rates and robustness (L9). Towards this end, training of a fleet of robots by human supervisors that are queried interactively to offer supplementary demonstrations when robots face failures is a promising strategy to increase the robustness of learning approaches at a moderate increase of costs. A pivotal aspect of this process is the efficient allocation of human experts to specific robot cells. A recent study by Hoque et al. [46] introduced heuristic query functions to discern the priority of queries emanating from robot cells. These heuristics, while useful, are tailored primarily for more straightforward tasks that have a limited solution scope. For more intricate multi-object manipulation tasks, there exists the flexibility to adjust the sequence of manipulations. This adaptability can be instrumental in better strategizing for manipulations that might pose challenges when executed autonomously. To harness this, there's a pressing need to devise skill learning techniques that are attuned to both uncertainty [CITE] and proficiency [CITE], thereby facilitating more precise and immediate querying of operator feedback. Yet, these methods have not been used yet for interactive learning in in a fleet of robots in an industrial context.

\subsection{Methodology and Organization}
SMARTeach is designed as holistic approach for intuitive and interactive robot teaching of complex manipulations. It aims at building new intuitive platforms for robot teaching as well as fundamental algorithmic developments for interactive learning of robot perception, scene modelling and learning with human feedback. All these algorithms will be considered in several complex use cases of complex manipulations of a large number of mixed objects which also includes understanding of visual and verbal instructions. 
% \vspace{-0.3cm}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\paragraph{WP1: Collecting human and synthetic data.} WP1 is centered on creating intuitive robot teaching interfaces, leveraging teleoperation and AR technologies to allow even novices to perform complex manipulations and generate training data efficiently. 
\begin{enumerate}[noitemsep, topsep=0pt, partopsep=0pt, label=\alph*), leftmargin=0em, itemindent=1em, labelindent=1em, labelwidth=*]

\item %
\textit{\textbf{Bimanual Interactive Manipulation Platforms:}} We will develop bimanual teleoperation systems with AR capabilities for three robot platforms, including industrial robots with parallel jaw grippers and the advanced Shadow platform. These systems will focus on nuanced control and full 3D tele-presence capabilities. 

\item %
\textit{\textbf{ Auto-generated Physics Simulators:}} We want to automaticall import physical objects observed in the scene into an physics simulation environment. We will auto-generate meshes and physical properties from the sensor data and human feedback in an interactive manner for easy import into simulation environments, facilitating the creation of tailored simulations and simulation-based optimization.
% \item \textit{\textbf{Crowd-Sourcing with Physical Simulators:}} Accurate physical simulators will be developed for each robot platform. Additionally, we will explore using cost-effective AR/VR gaming devices for remote operation, aiming to facilitate crowd-sourced demonstration collection via the internet. 

\item %
\textit{\textbf{AR Interface for Scene Comprehension and Correction:}} The AR interface will be enhanced for both visualization and user-driven correction of the robot’s environmental perception. This includes improving semantic segmentation and 6D pose estimation, and elucidating semantic object relations. 

\item %
\textit{\textbf{Interfaces for Skill Correction:}} Algorithms will be developed to allow human operators to seamlessly intervene and correct the skill execution using the tele-operation interfaces as well as for providing interfaces for visualizing.
\end{enumerate}
% \vspace{-0.3cm}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\paragraph{WP2: Few-Shot Adaptation of Foundational Representations.} In WP2, we focus on meta-learning for rapid fine-tuning of foundation models, develop generalizable foundational perception models using neural-symbolic architectures, and enhance learned representations by incorporating geometric modeling for improved understanding of object interactions during manipulation.
\begin{enumerate}[noitemsep, topsep=0pt, partopsep=0pt, label=\alph*), leftmargin=0em, itemindent=1em, labelindent=1em, labelwidth=*]
%
\item \textit{\textbf{Meta-Learning for Fine-Tuning Foundation Models:}} 
Building on existing foundation models for scene understanding \cite{SAM} and robot behavior prediction \cite{rtx2023arxiv}, we will apply Bayesian meta-learning algorithms for rapid and robust fine-tuning, while destilling the knowledge-base from the foundational model into the meta-learned models. 
%
\item \textit{\textbf{Foundational Perception Models:}}
 We aim to develop generalizable representations that transfer
well to new scenarios and can be adapted to online user feedback in terms of 6D-pose estimation, semantic segmentation and semantic object relations. 
%
\item \textit{\textbf{Foundational Object Interactions Models:}}
We will build foundational object interaction models that are pre-trained on a vast dataset of simulated objects ranging from rigid, deformable, and articulated objects and can be fine-tuned using meta-learning on real sensor data such as point-clouds and images. This will be based on our prior work on mesh-based geometric modeling of deformable objects using graph neural networks \cite{linkerhagnerFSM23}.
\end{enumerate}
% \vspace{-0.3cm}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\paragraph{WP3: Scaling-up Robot Skill-Learning Capabilities.} In WP3, we aim to develop hierarchical manipulation skill libraries that scale to scenes with many objects by dividing the learning problem into selecting the object to manipulate and executing a manipulation targeted at the selected object. We will further exploit geometric representations to generalize the skill libraries to multi-finger end-effectors and tool usage of various shapes. These representations will be learn-able from demonstrations and fine-tuneable using novel efficient off-policy and offline RL methods. We will further exploit simulation-based pre-tuning to identify potential solutions and initialize the skill-libraries. Promising solutions will be transferred to the real robot and fine-tuned using learned models in a data-efficient manner. 
\begin{enumerate}[noitemsep, topsep=0pt, partopsep=0pt, label=\alph*), leftmargin=0em, itemindent=1em, labelindent=1em, labelwidth=*]
\item \textit{\textbf{Object-Centric Manipulation Skill Libraries:}} We will develop new hierarchical skill representations that can divide the learning problem into learning which obect in the scene to manipulate and which manipulation to apply on a higher decision level, while the skill execution level learn to manipulate the selected target object in an object-centric manner. This hierarchy will support dealing with many objects while the object-centric skill representation provides good generalization properties to new object configurations.
%
\item \textit{\textbf{Endeffector and Tool-Geometry Aware Representations:}} This task aims to master dexterous manipulation with 5-finger hands and complex tools. We will embed tool and end-effector geometry in the scene representation and learn  score-functions, which depend on the geometry of the scene including end-effector and tool, from demonstrations. Such approach will allow us to plan manipulations with high-dimensional manipulators and various tool geometries. 
\item \textit{\textbf{ Off-Policy and Offline RL for Skill Libraries:}}
 We aim to create novel off-policy and offline RL algorithms that are suitable for learning smooth skill architectures. These algorithms will unify imitation learning and reinforcement learning to refine policies learned from demonstration data, enhancing their applicability and efficiency.
 %
\item \textit{\textbf{Integrated Simulation-Based Pre-Tuning and Model-Based Fine-Tuning:}}
Building on our prior work for discovering versatile solutions \cite{celik2022specializing}, multiple solutions for each task will be discovered in simulation and refined on the reals system using model-based reinforcement learning with dynamically updated object models, ensuring robustness and adaptability. This process will also incorporate Bayesian Meta-Learning to accurately capture uncertainties, enhancing the precision of skill tuning..
%
\end{enumerate}
% \vspace{-0.3cm}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\paragraph{WP4: Learning from Interactive Human Feedback and Offline Instructions.} WP4 is devoted to enhancing the interactive fine-tuning of object-centric manipulation skills, eliminating the reliance on extensively engineered task-specific reward functions. Our approach involves the development of versatile, object-centric reward representations, adaptable to various human feedback types, including demonstrations, comparisons, haptic and verbal corrections.
\begin{enumerate}[noitemsep, topsep=0pt, partopsep=0pt, label=\alph*), leftmargin=0em, itemindent=1em, labelindent=1em, labelwidth=*]
\item \textit{\textbf{Forming Generic Reward Representations:}} 
% This subtask involves creating object-centric reward representations using demonstrations, pair-wise comparisons, and scorings. The focus here is to evaluate manipulation trajectories in accordance with object coordinate frames. Foundation models will initially act as a baseline for reward structures, which will then be fine-tuned through human feedback. 
This task creates versatile reward representations for objects using local scene geometry. It refines reward models through adversarial methods, score-based diffusion, and human feedback via pairwise comparisons and corrections. I will further extend this approach to foundational reward models based on language descriptions, utilizing visual language models for interactive fine-tuning.
\item \textit{\textbf{Incorporating Verbal Corrections:}}
 % We aim to provide users with intuitive correction mechanisms for robot behavior, which could be through haptic feedback using the shared control interface developed in WP1, or via verbal instructions (e.g., "you did not push hard enough"). Integration of large language models into the reward framework will enhance this interactive process. 
 This task integrates verbal corrections into behavior analysis using tele-operation feedback. Annotated with language descriptions, a visual language model predicts corrections during skill execution, aiding in real-time improvement and updating policy and reward models upon success.
\item \textit{\textbf{Learning from Visual and Language Instructions:}}
% This task encompasses the parsing of instruction manuals into discrete segments of visual and linguistic instructions. The parsed content will significantly influence the long-term planning hierarchy within the skill representation framework, with an emphasis on refining skill and object selection policies. Initially, interpreting these manuals using only foundational knowledge may prove challenging. To address this, we will employ our interactive teaching methods to enable the robot to understand and process a variety of instruction manuals. 
This task aims to teach robots to parse and understand instruction manuals consisting of visual and linguistic instructions. It involves identifying differences in subsequent steps of provided building plans to estimate which object should be placed at which location. The approach expands on interpreting sketched target configurations and extends interactive teaching methods to effectively process diverse instruction manuals such that unseen tasks can be accomplished only by parsing instruction manual.
\end{enumerate}
% \vspace{-0.3cm}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\paragraph{WP5: Use Cases and User Studies.}
In WP5, we will focus on three highly challenging use cases: Lego sort and disassembly, folding paper into Oregami using instruction sheets, and Lego assembly with instruction manuals. Alongside, extensive user studies will evaluate the AR interfaces and interactive teaching algorithms.
\begin{enumerate}[noitemsep, topsep=0pt, partopsep=0pt, label=\alph*), leftmargin=0em, itemindent=1em, labelindent=1em, labelwidth=*]
    \item \textit{\textbf{Lego Sort and Dissambly:}}
The task involves sorting and disassembling a box of Lego bricks, showcasing advancements in skill learning within cluttered environments with numerous objects. The specific challenge is to understand complex highly cluttered scenes as well as the concept of composite objects that need to be disassembled  objects. The task will be executed using Franka Emika robots.
    \item \textit{\textbf{Folding Paper into Origami:}} The task is folding origami sculptures of different difficulty levels using instruction sheets. Objectives include modeling deformable objects, manipulation with dexterous hands and understanding visual and language instructions. 
    \item \textit{\textbf{Lego Assembly with Instruction Manuals:}} The task is assembling complex Lego structures using instruction manuals, showcasing skill learning in cluttered environments. It involves understanding assembly, disassembly of composite objects, and interpreting multi-step instructions.
    % \item \textit{\textbf{User Studies:}} Intensive studies will be conducted to assess the most effective control and AR interfaces and feedback types for these scenarios.
\end{enumerate}








%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \paragraph{WP2: Interactive Learning for Enhanced Perception and Modelling.} In WP2, we focus on developing interactive foundation models that utilize large offline datasets and quickly adapt to online user annotations and corrections. These models will improve scene understanding (6D object poses, pixel-wise segmentations) and predictive geometric modeling of objects.
% \begin{enumerate}[noitemsep, topsep=0pt, partopsep=0pt, label=\alph*), leftmargin=0em, itemindent=1em, labelindent=1em, labelwidth=*]
% \item \textit{\textbf{Online Integration of User Annotations and Corrections:}} Building on existing foundation models [CITE] for scene understanding and 6D pose estimation, we will apply Bayesian meta-learning algorithms for rapid and robust fine-tuning, incorporating latent task descriptors for object models. 
% \item \textit{\textbf{Learning Neuro-Symbolic Representations:}} We aim to develop generalizable representations that transfer well between robot architectures, using neural-symbolic architectures to learn and adjust object relations in real-time based on user input. 
% \item \textit{\textbf{ Predictive Modeling of Object Dynamics:}} We'll enhance the learned representation by learning how rigid, deformable, and articulated objects change during manipulation using geometric modelling CITE and hence, improving geometric understanding of the scene. 
% \item  \textit{\textbf{ Auto-generated Physics Simulators:}} The learned geometric and physical models will be integrated into physics simulations such as Isaac Sim. We will auto-generate meshes and physical properties from the sensor data and  annotations for easy import into simulation environments, facilitating the creation of tailored simulations and simulation-based optimization (see WP3) for a new use-case.
% \end{enumerate}
% \vspace{-0.3cm}
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \paragraph{WP3: Scaling Up Manipulation Skill Learning.} 
% WP3 focuses on advancing manipulation skill learning algorithms for scenarios involving numerous objects and complex interactions like insertion, screwing, and assembly. We aim to enhance data efficiency through cutting-edge off-policy and offline reinforcement learning (RL) algorithms, sim-to-real transfer, and model-based fine-tuning.
% \begin{enumerate}[noitemsep, topsep=0pt, partopsep=0pt, label=\alph*), leftmargin=0em, itemindent=1em, labelindent=1em, labelwidth=*]
% \item \textit{\textbf{Object-Centric Manipulation Skill Libraries:}} Building on our previous work with movement primitives, we'll develop new imitation learning algorithms for hierarchical skill representations that handle many objects. This object-centric architecture, integrating scene understanding from WP2, will efficiently select and manipulate objects based on their coordinates, scaling effectively to new object poses. 
% \item \textit{\textbf{ Off-Policy and Offline RL for Skill Libraries:}} For this new skill architecture, we'll create novel off-policy and offline RL algorithms that unify imitation learning and reinforcement learning to refine policies learned from demonstration data, enhancing their applicability and efficiency.
% \item \textit{\textbf{ Simulation-Based Pre-Tuning:}} Given the ease of policy learning in simulations, we will employ max-ent RL approaches [CITE] to make these policies more resilient to the sim-to-real gap. Efforts will also focus on adapting simulations to better reflect real-world conditions. 
% \item \textit{\textbf{ Model-Based Fine-Tuning:}} Leveraging our prior work in model learning and object models from WP2, we'll develop architectures for multi-object manipulations. These models will be instrumental in optimizing and refining the skill libraries efficiently.
% \end{enumerate}
% \vspace{-0.3cm}
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \paragraph{WP4: Learning from Human Feedback and Visual and Language Instructions}
% WP4 is dedicated to developing methods for interactively fine-tuning object-centric manipulation skills, circumventing the need for extensively engineered reward functions. This process will leverage a variety of feedback types from human operators, including demonstrations, comparisons, haptic and verbal corrections, to develop adaptable reward representations. Additionally, robot task knowledge will be enriched by utilizing offline datasets and learning to interpret instruction manuals. 
% \begin{enumerate}[noitemsep, topsep=0pt, partopsep=0pt, label=\alph*), leftmargin=0em, itemindent=1em, labelindent=1em, labelwidth=*]
% \item \textit{\textbf{Generic Reward Representations from Multiple Feedback Sources:}} This subtask involves creating object-centric reward representations using demonstrations, pair-wise comparisons, and scorings. The focus here is to evaluate manipulation trajectories in accordance with object coordinate frames. Foundation models will initially act as a baseline for reward structures, which will then be fine-tuned through human feedback. 
% \item \textit{\textbf{Incorporating Haptic and Verbal Corrections:}} We aim to provide users with intuitive correction mechanisms for robot behavior, which could be through haptic feedback using the shared control interface developed in WP1, or via verbal instructions (e.g., "you did not push hard enough"). Integration of large language models into the reward framework will enhance this interactive process. 
% \item \textit{\textbf{Uncertainty-Aware Reward Representations:}} In this section, robots will be trained to identify errors and unmastered skills, leading to more directed queries to human operators. Error recognition models will be developed using human feedback, combined with Bayesian Deep Learning techniques to incorporate uncertainty awareness into these models. These uncertainty estimates will then be utilized to guide the query generation for human operators. 
% \item \textit{\textbf{Learning from Visual and Language Instructions:}} This task encompasses the parsing of instruction manuals into discrete segments of visual and linguistic instructions. The parsed content will significantly influence the long-term planning hierarchy within the skill representation framework, with an emphasis on refining skill and object selection policies. Initially, interpreting these manuals using only foundational knowledge may prove challenging. To address this, we will employ our interactive teaching methods to enable the robot to understand and process a variety of instruction manuals. 
% \end{enumerate}

% \vspace{-0.3cm}
% \paragraph{WP5: Use Cases and User Studies}
% In WP5, we will focus on three high-impact use cases: searching and recovering objects, laundry sorting and folding, and the assembly of complex structures. Alongside, extensive user studies will evaluate the AR interfaces and interactive teaching algorithms.

% \begin{enumerate}[noitemsep, topsep=0pt, partopsep=0pt, label=\alph*), leftmargin=0em, itemindent=1em, labelindent=1em, labelwidth=*]
%     \item \textit{\textbf{Search and Recover Objects:}} Implemented with bimanual Franka Panda arms and an AR tele-operation system, this use case involves retrieving specific objects from a cluttered bin. To do so, the robot needs to get a good understanding of the cluttered scene and the involved objects. 
%     \item \textit{\textbf{Laundry Sorting and Folding:}} Utilizing the humanoid robot Armar 7 and tele-operation via motion controllers and data-gloves, this task focuses on sorting and folding laundry, with visual folding instructions as guide.
%     \item \textit{\textbf{Assembly of Complex Structures:}} This task will see the assembly of complex structures, such as Lego or Duplo models, following instruction manuals, using the shadow teleoperation platform for fine manipulation.
%     \item \textit{\textbf{User Studies:}} Intensive studies will be conducted to assess the most effective control and AR interfaces and feedback types for these scenarios.
% \end{enumerate}

%\paragraph{WP5: Use cases and User Studies.} In this WP, we will design 3 challenging use cases, i.e., industrial assembly, sorting and folding the laundry out of a full basket and assembly of complex "Lego-structures" following instruction manuals. Moreover, we will perform extensive user studies for the AR interfaces and interactive teaching algorithms. 
%\begin{enumerate}[noitemsep, topsep=0pt, partopsep=0pt, label=(\roman*), leftmargin=0em, itemindent=2em, labelindent=2em, labelwidth=*]
%\item \textit{\textbf{Industrial Assembly:} } This use-case will be implemented using bimanual Franka Panda arms and the tele-operation AR system illustrated in CITE. The robot has to plug in cables, screw, bolt and insert parts. The scenario will be developed in close collaboration with the start-up Artiminds CITE, who have access to real industrial use cases and where I already established a fruitful collaboration via joint paper submissions.
%\item \textit{\textbf{Search and Recover Objects:} }  This use-case will be implemented using bimanual Franka Panda arms and the tele-operation AR system illustrated in CITE. The robot needs to search and collect all objects of a given type from a clutterd bin containing a multitude of different objects. Objects need to be pushed away, grasped and removed from the bin. 
%\item \textit{\textbf{Laundry Sorting and Folding:} } This use-case will be implemented using the humanoid robot Armar 7 developed by the H2T team in Karlsruhe.  Tele-operation will be implemented via motion controllers and data-gloves to operate the fingers. The use-case will be developed in close collaboration with H2T, which is one of the leading robotics labs in Europe. The robot has to take clothes from the laundry basket, inspect them and fold them according the corresponding cloth category. In this task, we will also use visual instructions readily available in the internet of how to fold clothes as prior knowledge.  
%\item \textit{\textbf{Assembly of Complex Lego-Structures following Instruction Manuals:}} In this task, we require dexterous hands to manipulate small lego bricks such that the robot can build complex lego structures following instruction manuals. We will utilize the shadow teleoperation platform for doing so and teach the robot how to follow a diverse set of instruction manuals. In case the lego bricks are too fine grained, we will utilize larger bricks such as Lego Duplo. There exists over 300 instruction manuals for Lego Duplo and even more for standard Lego bricks. 
%\item \textit{\textbf{User Studies:}} We will perform intensive user studies to test what are the best control interfaces, AR interfaces and feedback types for these use cases.
%\end{enumerate}

\subsection{Impact}
\textbf{Scientific Impact:} SMARTeach marks a significant step forward in interactive learning and cognitive robotics. This project is not just about specific tasks; it's about setting a new standard in robotics capabilities across a spectrum of applications. By enhancing robot teaching interfaces and manipulation skills, SMARTeach will contribute to foundational advancements in AI, with implications for reinforcement learning, human-robot interaction, and control theory. The generic nature of our approach means that while we focus on examples like industrial assembly, logistics, and service robotics, the methodologies and technologies developed will have far-reaching applications in a multitude of experimental sciences and industries. \newline
\textbf{Industrial Impact:} SMARTeach will have a profound impact on the industrial sector, particularly in areas requiring precise and complex manipulation such as assembly lines. The project's advancements in intuitive robot teaching and scalable learning algorithms will enable more efficient, accurate, and flexible robotic systems, applicable to a wide range of industrial tasks beyond the specific examples of gearboxes or cables assembly. This will not only enhance productivity and quality but also address key challenges such as workforce shortages and workplace safety.
Beyond industrial applications, SMARTeach's technologies are poised to transform logistics and service sectors. For logistics, the ability to efficiently locate and package a variety of items will revolutionize warehouse operations. In service sectors, particularly in tasks like laundry handling, SMARTeach will pave the way for innovative robotic solutions, enhancing efficiency and user experience. \newline
\textbf{Societal Impact:} The overarching goal of SMARTeach is to provide technological solutions that address societal needs. By automating complex tasks, the project will alleviate labor shortages and contribute to safer work environments. The commitment to open-source, reproducible technology underlines SMARTeach’s dedication to accessible and equitable technological progress. In the long run, SMARTeach's contributions will not only streamline operations across various sectors but also support sustainable practices and enhance quality of life, demonstrating the project’s extensive societal benefits.

\subsection{Risk Assessment and Chances for Success}

SMARTeach, while high-risk, is counterbalanced by substantial risk mitigation measures. The project's resilience stems from its versatile approach, utilizing diverse feedback signals to ensure robustness even if certain feedback mechanisms are less effective. We have contingency plans for teleoperation interfaces, ready to switch to alternative technologies like motion controllers or haptic joysticks if necessary. Our project's strength is further bolstered by strong collaborations with leading industries such as Bosch,  and innovative startups like Artiminds, as well as with prominent service robotics labs like H2T at the KIT. These partnerships are crucial in driving our use-case design, aligning our research with real-world applications. My personal long-term commitment to transforming reinforcement learning and robotics, particularly in fields like industrial assembly and household robotics, underscores the project's potential for impactful, sustained success.

\subsection{Resources and commitment of the PI}
I will commit 40\% of my time to SMARTeach. The personnel costs will go to 3 PhD students (4 years each) and 1 PostDoc position (3 years). In addition, I apply for funding for the Shadow Teleoperation System (500k€) that will be fully integrated in our interactive AR tele-operation pipeline. %and 3 low-cost Aloha platforms from Trossen robotics (24k€ each) to extend our approach to robot fleets and scale experimentation. 
% Add \subsection's as needed. It is possible to refer to a part of the
% B2 using \input{../part-B2/parts/file}; within this file, \ifbone can
% be used to determine if one is in the B1 or in the B2



% Add \nocite{} for any citation from the CV or EATR to add to the
% bibliography

\pagebreak
\defbibheading{subsection}[\bibname]{\subsection*{#1}}
\printbibliography[prenote=bolditalics,heading=subsection]

\end{refsection}
