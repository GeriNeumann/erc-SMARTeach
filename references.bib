% Put BibTeX entries other than your own work in this file
 @inproceedings{Bushman_Asselmeier_Won_LaViers_2020, title={Toward Human-like Teleoperated Robot Motion: Performance and Perception of a Choreography-inspired Method in Static and Dynamic Tasks for Rapid Pose Selection of Articulated Robots}, ISSN={2577-087X}, DOI={10.1109/ICRA40945.2020.9196742}, abstractNote={In some applications, operators may want to create fluid, human-like motion on a remotely-operated robot, for example, a device used for remote telepresence. This paper examines two methods of controlling the pose of a Baxter robot via an Xbox One controller. The first method is a joint- by-joint (JBJ) method in which one joint of each limb is specified in sequence. The second method of control, named Robot Choreography Center (RCC), utilizes choreographic abstractions in order to simultaneously move multiple joints of the limb of the robot in a predictable manner. Thirty-eight users were asked to perform four tasks with each method. Success rate and duration of successfully completed tasks were used to analyze the performances of the participants. Analysis of the preferences of the users found that the joint-by-joint (JBJ) method was considered to be more precise, easier to use, safer, and more articulate, while the choreography-inspired (RCC) method of control was perceived as faster, more fluid, and more expressive. Moreover, performance data found that while both methods of control were over 80% successful for the two static tasks, the RCC method was an average of 11.85% more successful for the two more difficult, dynamic tasks. Future work will leverage this framework to investigate ideas of fluidity, expressivity, and human-likeness in robotic motion through online user studies with larger participant pools.}, booktitle={2020 IEEE International Conference on Robotics and Automation (ICRA)}, author={Bushman, A. and Asselmeier, M. and Won, J. and LaViers, A.}, year={2020}, month={May}, pages={10219–10225} }

@InProceedings{pvn3d,
author = {He, Yisheng and Sun, Wei and Huang, Haibin and Liu, Jianran and Fan, Haoqiang and Sun, Jian},
title = {PVN3D: A Deep Point-Wise 3D Keypoints Voting Network for 6DoF Pose Estimation},
booktitle = {IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
month = {6},
year = {2020}
} 

@inproceedings{xu2023iql,
  title={IQL-TD-MPC: Implicit Q-Learning for Hierarchical Model Predictive Control},
  author={Xu, Yingchen and Chitnis, Rohan and Hashemi, Bobak T and Lehnert, Lucas and Dogan, Urun and Zhu, Zheqing and Delalleau, Olivier},
  booktitle={ICML Workshop on New Frontiers in Learning, Control, and Dynamical Systems},
  year={2023}
}

@article{hafner2022deep,
  title={Deep hierarchical planning from pixels},
  author={Hafner, Danijar and Lee, Kuang-Huei and Fischer, Ian and Abbeel, Pieter},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={26091--26104},
  year={2022}
}


@InProceedings{laskin20a,
  title = 	 {{CURL}: Contrastive Unsupervised Representations for Reinforcement Learning},
  author =       {Laskin, Michael and Srinivas, Aravind and Abbeel, Pieter},
  booktitle = 	 {Proceedings of the 37th International Conference on Machine Learning},
  pages = 	 {5639--5650},
  year = 	 {2020},
  editor = 	 {III, Hal Daumé and Singh, Aarti},
  volume = 	 {119},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {13--18 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v119/laskin20a/laskin20a.pdf},
  url = 	 {https://proceedings.mlr.press/v119/laskin20a.html},
  abstract = 	 {We present CURL: Contrastive Unsupervised Representations for Reinforcement Learning. CURL extracts high-level features from raw pixels using contrastive learning and performs off-policy control on top of the extracted features. CURL outperforms prior pixel-based methods, both model-based and model-free, on complex tasks in the DeepMind Control Suite and Atari Games showing 1.9x and 1.2x performance gains at the 100K environment and interaction steps benchmarks respectively. On the DeepMind Control Suite, CURL is the first image-based algorithm to nearly match the sample-efficiency of methods that use state-based features. Our code is open-sourced and available at https://www.github.com/MishaLaskin/curl.}
}

@article{ContrastivePredictive,
  author       = {A{\"{a}}ron van den Oord and
                  Yazhe Li and
                  Oriol Vinyals},
  title        = {Representation Learning with Contrastive Predictive Coding},
  journal      = {CoRR},
  volume       = {abs/1807.03748},
  year         = {2018},
  url          = {http://arxiv.org/abs/1807.03748},
  eprinttype    = {arXiv},
  eprint       = {1807.03748},
  timestamp    = {Mon, 13 Aug 2018 16:48:25 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-1807-03748.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{DINOv2,
  title={DINOv2: Learning Robust Visual Features without Supervision},
  author={Maxime Oquab and Timoth'ee Darcet and Th{\'e}o Moutakanni and Huy Q. Vo and Marc Szafraniec and Vasil Khalidov and Pierre Fernandez and Daniel Haziza and Francisco Massa and Alaaeldin El-Nouby and Mahmoud Assran and Nicolas Ballas and Wojciech Galuba and Russ Howes and Po-Yao (Bernie) Huang and Shang-Wen Li and Ishan Misra and Michael G. Rabbat and Vasu Sharma and Gabriel Synnaeve and Huijiao Xu and Herv{\'e} J{\'e}gou and Julien Mairal and Patrick Labatut and Armand Joulin and Piotr Bojanowski},
  journal={ArXiv},
  year={2023},
  volume={abs/2304.07193},
}

@article{RePOSE,
  title={RePOSE: Fast 6D Object Pose Refinement via Deep Texture Rendering},
  author={Shun Iwase and Xingyu Liu and Rawal Khirodkar and Rio Yokota and Kris M. Kitani},
  journal={2021 IEEE/CVF International Conference on Computer Vision (ICCV)},
  year={2021},
  pages={3283-3292},
}
@inproceedings{Zeng2022AreTE,
  title={Are Transformers Effective for Time Series Forecasting?},
  author={Ailing Zeng and Muxi Chen and Lei Zhang and Qiang Xu},
  journal={Proceedings of the AAAI Conference on Artificial Intelligence},
  year={2023}
}

@inproceedings{
raffin2021smooth,
title={Smooth Exploration for Robotic Reinforcement Learning},
author={Antonin Raffin and Jens Kober and Freek Stulp},
booktitle={5th Annual Conference on Robot Learning },
year={2021},
url={https://openreview.net/forum?id=TSuSGVkjuXd}
}

@article{chen2020category,
  title={Category Level Object Pose Estimation via Neural Analysis-by-Synthesis},
  author={Chen, Xu and Dong, Zijian, and Song, Jie and Geiger, Andreas and Hilliges, Otmar},
  year= {2020},
  booktitle = {European Conference on Computer Vision (ECCV)},
}

@InProceedings{AminiNaieni23,
  author       = "Amini-Naieni, N. and Amini-Naieni, K. and Han, T. and Zisserman, A.",
  title        = "Open-world Text-specified Object Counting",
  booktitle    = "British Machine Vision Conference",
  year         = "2023",
}

@inproceedings{OVSeg,
  title={Open-vocabulary semantic segmentation with mask-adapted clip},
  author={Liang, Feng and Wu, Bichen and Dai, Xiaoliang and Li, Kunpeng and Zhao, Yinan and Zhang, Hang and Zhang, Peizhao and Vajda, Peter and Marculescu, Diana},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={7061--7070},
  year={2023}
}

@article{Inerf,
  title={iNeRF: Inverting Neural Radiance Fields for Pose Estimation},
  author={Yen-Chen Lin and Peter R. Florence and Jonathan T. Barron and Alberto Rodriguez and Phillip Isola and Tsung-Yi Lin},
  journal={2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
  year={2020},
  pages={1323-1330},
}

@inproceedings{liu2022gen6d,
  title={Gen6D: Generalizable Model-Free 6-DoF Object Pose Estimation from RGB Images},
  author={Liu, Yuan and Wen, Yilin and Peng, Sida and Lin, Cheng and Long, Xiaoxiao and Komura, Taku and Wang, Wenping},
  booktitle={ECCV},
  year={2022}
}

@inproceedings{UOC,
  title={Learning RGB-D Feature Embeddings for Unseen Object Instance Segmentation},
  author={Yu Xiang and Christopher Xie and Arsalan Mousavian and Dieter Fox},
  booktitle={Conference on Robot Learning},
  year={2020},
}

@InProceedings{He_2017_ICCV,
author = {He, Kaiming and Gkioxari, Georgia and Dollar, Piotr and Girshick, Ross},
title = {Mask R-CNN},
booktitle = {Proceedings of the IEEE International Conference on Computer Vision (ICCV)},
month = {10},
year = {2017}
}

@article{Losey2019,
author = {Losey, Dylan P. and O'Malley, Marcia K.},
title = {Learning the Correct Robot Trajectory in Real-Time from Physical Human Interactions},
year = {2019},
issue_date = {March 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {1},
url = {https://doi.org/10.1145/3354139},
doi = {10.1145/3354139},
abstract = {We present a learning and control strategy that enables robots to harness physical human interventions to update their trajectory and goal during autonomous tasks. Within the state of the art, the robot typically reacts to physical interactions by modifying a local segment of its trajectory, or by searching for the global trajectory offline, using either replanning or previous demonstrations. Instead, we explore a one-shot approach: here, the robot updates its entire trajectory and goal in real time without relying on multiple iterations, offline demonstrations, or replanning. Our solution is grounded in optimal control and gradient descent, and extends linear-quadratic regulator controllers to generalize across methods that locally or globally modify the robot’s underlying trajectory. In the best case, this Linear-quadratic regulator + Learning approach matches the optimal offline response to physical interactions, and—in more challenging cases—our strategy is robust to noisy and unexpected human corrections. We compare the proposed solution against other real-time strategies in a user study and demonstrate its efficacy in terms of both objective and subjective measures.},
journal = {J. Hum.-Robot Interact.},
month = {dec},
articleno = {1},
numpages = {19},
keywords = {physical human-robot interaction, Learning from demonstrations, optimal control}
}

@InProceedings{SAM,
    author    = {Kirillov, Alexander and Mintun, Eric and Ravi, Nikhila and Mao, Hanzi and Rolland, Chloe and Gustafson, Laura and Xiao, Tete and Whitehead, Spencer and Berg, Alexander C. and Lo, Wan-Yen and Dollar, Piotr and Girshick, Ross},
    title     = {Segment Anything},
    booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)},
    month     = {10},
    year      = {2023},
    pages     = {4015-4026}
}

@article{DenseFusion,
  title={DenseFusion: 6D Object Pose Estimation by Iterative Dense Fusion},
  author={Chen Wang and Danfei Xu and Yuke Zhu and Roberto Mart{\'i}n-Mart{\'i}n and Cewu Lu and Li Fei-Fei and Silvio Savarese},
  journal={2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  year={2019},
  pages={3338-3347},
}

 @article{Lipton_Fay_Rus_2018, title={Baxter’s Homunculus: Virtual Reality Spaces for Teleoperation in Manufacturing}, volume={3}, ISSN={2377-3766}, DOI={10.1109/LRA.2017.2737046}, abstractNote={We demonstrate a low-cost telerobotic system that leverages commercial virtual reality (VR) technology and integrates it with existing robotics control infrastructure. The system runs on a commercial gaming engine using off-the-shelf VR hardware and can be deployed on multiple network architectures. The system is based on the homunculus model of mind wherein we embed the user in a VR control room. The control room allows for multiple sensor displays, and dynamic mapping between the user and robot. This dynamic mapping allows for selective engagement between the user and the robot. We compared our system with state-of-the-art automation algorithms and standard VR-based telepresence systems by performing a user study. The study showed that new users were faster and more accurate than the automation or a direct telepresence system. We also demonstrate that our system can be used for pick and place, assembly, and manufacturing tasks.}, number={1}, journal={IEEE Robotics and Automation Letters}, author={Lipton, Jeffrey I. and Fay, Aidan J. and Rus, Daniela}, year={2018}, month={Jan}, pages={179–186} }
 
@article{FFB6D,
  title={FFB6D: A Full Flow Bidirectional Fusion Network for 6D Pose Estimation},
  author={Yisheng He and Haibin Huang and Haoqiang Fan and Qifeng Chen and Jian Sun},
  journal={2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  year={2021},
  pages={3002-3012},
} 

@article{wu2022slotformer,
  title={Slotformer: Unsupervised visual dynamics simulation with object-centric models},
  author={Wu, Ziyi and Dvornik, Nikita and Greff, Klaus and Kipf, Thomas and Garg, Animesh},
  journal={International Conferene On Learning Representations},
  year={2023}
}

@inproceedings{xiang2018posecnn,
    Author = {Yu Xiang and Tanner Schmidt and Venkatraman Narayanan and Dieter Fox},
    Title = {{PoseCNN}: A Convolutional Neural Network for {6D} Object Pose Estimation in Cluttered Scenes},
    booktitle = {Robotics: Science and Systems (RSS)},
    Year = {2018}
}
@inproceedings{finn2017model,
  title={Model-agnostic meta-learning for fast adaptation of deep networks},
  author={Finn, Chelsea and Abbeel, Pieter and Levine, Sergey},
  booktitle={International conference on machine learning},
  pages={1126--1135},
  year={2017},
  organization={PMLR}
}
@article{xian2021hyperdynamics,
  title={Hyperdynamics: Meta-learning object and agent dynamics with hypernetworks},
  author={Xian, Zhou and Lal, Shamit and Tung, Hsiao-Yu and Platanios, Emmanouil Antonios and Fragkiadaki, Katerina},
  journal={International Conference On Learning Representations},
  year={2021}
}

@article{chua2018deep,
  title={Deep reinforcement learning in a handful of trials using probabilistic dynamics models},
  author={Chua, Kurtland and Calandra, Roberto and McAllister, Rowan and Levine, Sergey},
  journal={Advances in neural information processing systems},
  volume={31},
  year={2018}
}
@article{gu2021efficiently,
  title={Efficiently modeling long sequences with structured state spaces},
  author={Gu, Albert and Goel, Karan and R{\'e}, Christopher},
  journal={arXiv preprint arXiv:2111.00396},
  year={2021}
}

@article{smith2022simplified,
  title={Simplified state space layers for sequence modeling},
  author={Smith, Jimmy TH and Warrington, Andrew and Linderman, Scott W},
  journal={arXiv preprint arXiv:2208.04933},
  year={2022}
}

@inproceedings{zhou2021informer,
  title={Informer: Beyond efficient transformer for long sequence time-series forecasting},
  author={Zhou, Haoyi and Zhang, Shanghang and Peng, Jieqi and Zhang, Shuai and Li, Jianxin and Xiong, Hui and Zhang, Wancai},
  booktitle={Proceedings of the AAAI conference on artificial intelligence},
  volume={35},
  number={12},
  pages={11106--11115},
  year={2021}
}

@inproceedings{zeng2023transformers,
  title={Are transformers effective for time series forecasting?},
  author={Zeng, Ailing and Chen, Muxi and Zhang, Lei and Xu, Qiang},
  booktitle={Proceedings of the AAAI conference on artificial intelligence},
  volume={37},
  number={9},
  pages={11121--11128},
  year={2023}
}

@article{lecun2022path,
  title={A path towards autonomous machine intelligence version 0.9. 2, 2022-06-27},
  author={LeCun, Yann},
  journal={Open Review},
  volume={62},
  number={1},
  year={2022}
}

@inproceedings{CLIP,
  title={Learning Transferable Visual Models From Natural Language Supervision},
  author={Alec Radford and Jong Wook Kim and Chris Hallacy and Aditya Ramesh and Gabriel Goh and Sandhini Agarwal and Girish Sastry and Amanda Askell and Pamela Mishkin and Jack Clark and Gretchen Krueger and Ilya Sutskever},
  booktitle={International Conference on Machine Learning},
  year={2021},
}

@article{LLaMA,
  title={LLaMA: Open and Efficient Foundation Language Models},
  author={Hugo Touvron and Thibaut Lavril and Gautier Izacard and Xavier Martinet and Marie-Anne Lachaux and Timoth{\'e}e Lacroix and Baptiste Rozi{\`e}re and Naman Goyal and Eric Hambro and Faisal Azhar and Aurelien Rodriguez and Armand Joulin and Edouard Grave and Guillaume Lample},
  journal={ArXiv},
  year={2023},
  volume={abs/2302.13971},
}

@inproceedings{
li2022languagedriven,
title={Language-driven Semantic Segmentation},
author={Boyi Li and Kilian Q Weinberger and Serge Belongie and Vladlen Koltun and Rene Ranftl},
booktitle={International Conference on Learning Representations},
year={2022},
url={https://openreview.net/forum?id=RriDjddCLN}
}

@InProceedings{SAMfinetuning,
author="Wang, An
and Islam, Mobarakol
and Xu, Mengya
and Zhang, Yang
and Ren, Hongliang",
editor="Celebi, M. Emre
and Salekin, Md Sirajus
and Kim, Hyunwoo
and Albarqouni, Shadi
and Barata, Catarina
and Halpern, Allan
and Tschandl, Philipp
and Combalia, Marc
and Liu, Yuan
and Zamzmi, Ghada
and Levy, Joshua
and Rangwala, Huzefa
and Reinke, Annika
and Wynn, Diya
and Landman, Bennett
and Jeong, Won-Ki
and Shen, Yiqing
and Deng, Zhongying
and Bakas, Spyridon
and Li, Xiaoxiao
and Qin, Chen
and Rieke, Nicola
and Roth, Holger
and Xu, Daguang",
title="SAM Meets Robotic Surgery: An Empirical Study on Generalization, Robustness and Adaptation",
booktitle="Medical Image Computing and Computer Assisted Intervention -- MICCAI 2023 Workshops ",
year="2023",
publisher="Springer Nature Switzerland",
address="Cham",
pages="234--244",
abstract="The Segment Anything Model (SAM) serves as a fundamental model for semantic segmentation and demonstrates remarkable generalization capabilities across a wide range of downstream scenarios. In this empirical study, we examine SAM's robustness and zero-shot generalizability in the field of robotic surgery. We comprehensively explore different scenarios, including prompted and unprompted situations, bounding box and points-based prompt approaches, as well as the ability to generalize under corruptions and perturbations at five severity levels. Additionally, we compare the performance of SAM with state-of-the-art supervised models. We conduct all the experiments with two well-known robotic instrument segmentation datasets from MICCAI EndoVis 2017 and 2018 challenges. Our extensive evaluation results reveal that although SAM shows remarkable zero-shot generalization ability with bounding box prompts, it struggles to segment the whole instrument with point-based prompts and unprompted settings. Furthermore, our qualitative figures demonstrate that the model either failed to predict certain parts of the instrument mask (e.g., jaws, wrist) or predicted parts of the instrument as wrong classes in the scenario of overlapping instruments within the same bounding box or with the point-based prompt. In fact, SAM struggles to identify instruments in complex surgical scenarios characterized by the presence of blood, reflection, blur, and shade. Additionally, SAM is insufficiently robust to maintain high performance when subjected to various forms of data corruption. We also attempt to fine-tune SAM using Low-rank Adaptation (LoRA) and propose SurgicalSAM, which shows the capability in class-wise mask prediction without prompt. Therefore, we can argue that, without further domain-specific fine-tuning, SAM is not ready for downstream surgical tasks.",
isbn="978-3-031-47401-9"
}

@InProceedings{Sun_2023_ICCV,
    author    = {Sun, Ximeng and Zhang, Pengchuan and Zhang, Peizhao and Shah, Hardik and Saenko, Kate and Xia, Xide},
    title     = {DIME-FM : DIstilling Multimodal and Efficient Foundation Models},
    booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)},
    month     = {October},
    year      = {2023},
    pages     = {15521-15533}
}

@inproceedings{NIPS2017_453fadbd,
 author = {Andrychowicz, Marcin and Wolski, Filip and Ray, Alex and Schneider, Jonas and Fong, Rachel and Welinder, Peter and McGrew, Bob and Tobin, Josh and Pieter Abbeel, OpenAI and Zaremba, Wojciech},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Hindsight Experience Replay},
 url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/453fadbd8a1a3af50a9df4df899537b5-Paper.pdf},
 volume = {30},
 year = {2017}
}

@misc{liu2023grounding,
      title={Grounding DINO: Marrying DINO with Grounded Pre-Training for Open-Set Object Detection}, 
      author={Shilong Liu and Zhaoyang Zeng and Tianhe Ren and Feng Li and Hao Zhang and Jie Yang and Chunyuan Li and Jianwei Yang and Hang Su and Jun Zhu and Lei Zhang},
      year={2023},
      eprint={2303.05499},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}




@inproceedings{DelPreto_Lipton_Sanneman_Fay_Fourie_Choi_Rus_2020, title={Helping Robots Learn: A Human-Robot Master-Apprentice Model Using Demonstrations via Virtual Reality Teleoperation}, ISSN={2577-087X}, DOI={10.1109/ICRA40945.2020.9196754}, abstractNote={As artificial intelligence becomes an increasingly prevalent method of enhancing robotic capabilities, it is important to consider effective ways to train these learning pipelines and to leverage human expertise. Working towards these goals, a master-apprentice model is presented and is evaluated during a grasping task for effectiveness and human perception. The apprenticeship model augments self-supervised learning with learning by demonstration, efficiently using the human’s time and expertise while facilitating future scalability to supervision of multiple robots; the human provides demonstrations via virtual reality when the robot cannot complete the task autonomously. Experimental results indicate that the robot learns a grasping task with the apprenticeship model faster than with a solely self-supervised approach and with fewer human interventions than a solely demonstration-based approach; 100% grasping success is obtained after 150 grasps with 19 demonstrations. Preliminary user studies evaluating workload, usability, and effectiveness of the system yield promising results for system scalability and deployability. They also suggest a tendency for users to overestimate the robot’s skill and to generalize its capabilities, especially as learning improves.}, booktitle={2020 IEEE International Conference on Robotics and Automation (ICRA)}, author={DelPreto, Joseph and Lipton, Jeffrey I. and Sanneman, Lindsay and Fay, Aidan J. and Fourie, Christopher and Choi, Changhyun and Rus, Daniela}, year={2020}, month={May}, pages={10226–10233} }

@inproceedings{Jang_Niu_Collins_Weightman_Carrasco_Lennox_2021, title={Virtual Kinesthetic Teaching for Bimanual Telemanipulation}, ISSN={2474-2325}, DOI={10.1109/IEEECONF49454.2021.9382763}, abstractNote={This paper proposes a novel telemanipulation system that enables a human operator to control a dual-arm robot. The operation provides kinesthetic teaching via a digital twin of the robot which the operator cyber-physically guides to perform a task. Its key enabler is the concept of a virtual reality interactive marker, which serves as a simplified end effector of the digital twin robot. In virtual reality, the operator can interact with the marker using bare hands, which are sensed by the Leap Motion on top of a virtual reality headset. Then, the status (e.g. position/orientation) of the marker is transformed to the corresponding joint space command to the remote robot so that its end effector can follow the marker. We provide the details of the system architecture, and implement the system based on commercial robots/devices (i.e. UR5, Robotiq gripper, Leap Motion), virtual reality, ROS, and Unity3D. Moreover, the paper discusses the technical challenges that we had to address, and the system’s potential benefits from a human-robot interaction perspective.}, booktitle={2021 IEEE/SICE International Symposium on System Integration (SII)}, author={Jang, Inmo and Niu, Hanlin and Collins, Emily C. and Weightman, Andrew and Carrasco, Joaquin and Lennox, Barry}, year={2021}, month={Jan}, pages={120–125} }

 @inproceedings{Das_Bechtle_Davchev_Jayaraman_Rai_Meier_2021, title={Model-Based Inverse Reinforcement Learning from Visual Demonstrations}, ISSN={2640-3498}, url={https://proceedings.mlr.press/v155/das21a.html}, abstractNote={Scaling model-based inverse reinforcement learning (IRL) to real robotic manipulation tasks with unknown dynamics remains an open problem. The key challenges lie in learning good dynamics models, developing algorithms that scale to high-dimensional state-spaces and being able to learn from both visual and proprioceptive demonstrations. In this work, we present a gradient-based inverse reinforcement learning framework that utilizes a pre-trained visual dynamics model to learn cost functions when given only visual human demonstrations. The learned cost functions are then used to reproduce the demonstrated behavior via visual model predictive control. We evaluate our framework on hardware on two basic object manipulation tasks.}, booktitle={Proceedings of the 2020 Conference on Robot Learning}, publisher={PMLR}, author={Das, Neha and Bechtle, Sarah and Davchev, Todor and Jayaraman, Dinesh and Rai, Akshara and Meier, Franziska}, year={2021}, month=oct, pages={1930–1942}, language={en} }

@inproceedings{wu2023learning,
  title={Learning Controllable Adaptive Simulation for Multi-resolution Physics},
  author={Wu, Tailin and Maruyama, Takashi and Zhao, Qingqing and Wetzstein, Gordon and Leskovec, Jure},
  booktitle={The Eleventh International Conference on Learning Representations},
  year={2023}
}

@inproceedings{Hedlund_Johnson_Gombolay_2021, address={Boulder CO USA}, title={The Effects of a Robot’s Performance on Human Teachers for Learning from Demonstration Tasks}, ISBN={978-1-4503-8289-2}, url={https://dl.acm.org/doi/10.1145/3434073.3444664}, DOI={10.1145/3434073.3444664}, booktitle={Proceedings of the 2021 ACM/IEEE International Conference on Human-Robot Interaction}, publisher={ACM}, author={Hedlund, Erin and Johnson, Michael and Gombolay, Matthew}, year={2021}, month={Mar}, pages={207–215}, language={en} }

 @inproceedings{Moorman_Hedlund-Botti_Schrum_Natarajan_Gombolay_2023, address={Stockholm Sweden}, title={Impacts of Robot Learning on User Attitude and Behavior}, ISBN={978-1-4503-9964-7}, url={https://dl.acm.org/doi/10.1145/3568162.3576996}, DOI={10.1145/3568162.3576996}, booktitle={Proceedings of the 2023 ACM/IEEE International Conference on Human-Robot Interaction}, publisher={ACM}, author={Moorman, Nina and Hedlund-Botti, Erin and Schrum, Mariah and Natarajan, Manisha and Gombolay, Matthew C.}, year={2023}, month={Mar}, pages={534–543}, language={en} }

@article{Mullen_Mosier_Chakrabarti_Chen_White_Losey_2021, title={Communicating Inferred Goals With Passive Augmented Reality and Active Haptic Feedback}, volume={6}, ISSN={2377-3766}, DOI={10.1109/LRA.2021.3111055}, abstractNote={Robots learn as they interact with humans. Consider a human teleoperating an assistive robot arm: as the human guides and corrects the arm’s motion, the robot gathers information about the human’s desired task. But how does the human know what their robot has inferred? Today’s approaches often focus on conveying intent: for instance, using legible motions or gestures to indicate what the robot is planning. However, closing the loop on robot inference requires more than just revealing the robot’s current policy: the robot should also display the alternatives it thinks are likely, and prompt the human teacher when additional guidance is necessary. In this letter we propose a multimodal approach for communicating robot inference that combines both passive and active feedback. Specifically, we leverage information-rich augmented reality to passively visualize what the robot has inferred, and attention-grabbing haptic wristbands to actively prompt and direct the human’s teaching. We apply our system to shared autonomy tasks where the robot must infer the human’s goal in real-time. Within this context, we integrate passive and active modalities into a single algorithmic framework that determines when and which type of feedback to provide. Combining both passive and active feedback experimentally outperforms single modality baselines; during an in-person user study, we demonstrate that our integrated approach increases how efficiently humans teach the robot while simultaneously decreasing the amount of time humans spend interacting with the robot. Videos here: https://youtu.be/swq_u4iIP-g}, number={4}, journal={IEEE Robotics and Automation Letters}, author={Mullen, James F. and Mosier, Josh and Chakrabarti, Sounak and Chen, Anqi and White, Tyler and Losey, Dylan P.}, year={2021}, month={10}, pages={8522–8529} }

 @inproceedings{Rosen_Whitney_Phillips_Chien_Tompkin_Konidaris_Tellex_2020, address={Cham}, series={Springer Proceedings in Advanced Robotics}, title={Communicating Robot Arm Motion Intent Through Mixed Reality Head-Mounted Displays}, ISBN={978-3-030-28619-4}, DOI={10.1007/978-3-030-28619-4_26}, abstractNote={Efficient motion intent communication is necessary for safe and collaborative work environments with collocated humans and robots. Humans efficiently communicate their motion intent to other humans through gestures, gaze, and social cues. However, robots often have difficulty efficiently communicating their motion intent to humans via these methods. Many existing methods for robot motion intent communication rely on 2D displays, which require the human to continually pause their work and check a visualization. We propose a mixed reality head-mounted display visualization of the proposed robot motion over the wearer’s real-world view of the robot and its environment. To evaluate the effectiveness of this system against a 2D display visualization and against no visualization, we asked 32 participants to labeled different robot arm motions as either colliding or non-colliding with blocks on a table. We found a 16% increase in accuracy with a 62% decrease in the time it took to complete the task compared to the next best system. This demonstrates that a mixed-reality HMD allows a human to more quickly and accurately tell where the robot is going to move than the compared baselines.}, booktitle={Robotics Research}, publisher={Springer International Publishing}, author={Rosen, Eric and Whitney, David and Phillips, Elizabeth and Chien, Gary and Tompkin, James and Konidaris, George and Tellex, Stefanie}, editor={Amato, Nancy M. and Hager, Greg and Thomas, Shawna and Torres-Torriti, Miguel}, year={2020}, pages={301–316}, collection={Springer Proceedings in Advanced Robotics}, language={en} }

% C1: 

@article{Wrede_Emmerich_Grünberg_Nordmann_Swadzba_Steil_2013, title={A User Study on Kinesthetic Teaching of Redundant Robots in Task and Configuration Space}, volume={2}, ISSN={21630364}, DOI={10.5898/JHRI.2.1.Wrede}, number={1}, journal={Journal of Human-Robot Interaction}, author={Wrede, Sebastian and Emmerich, Christian and Grünberg, Ricarda and Nordmann, Arne and Swadzba, Agnes and Steil, Jochen}, year={2013}, month={3}, pages={56–81} }

 @article{Ravichandar_Polydoros_Chernova_Billard_2020, title={Recent Advances in Robot Learning from Demonstration}, volume={3}, DOI={10.1146/annurev-control-100819-063206}, abstractNote={In the context of robotics and automation, learning from demonstration (LfD) is the paradigm in which robots acquire new skills by learning to imitate an expert. The choice of LfD over other robot learning methods is compelling when ideal behavior can be neither easily scripted (as is done in traditional robot programming) nor easily defined as an optimization problem, but can be demonstrated. While there have been multiple surveys of this field in the past, there is a need for a new one given the considerable growth in the number of publications in recent years. This review aims to provide an overview of the collection of machine-learning methods used to enable a robot to learn from and imitate a teacher. We focus on recent advancements in the field and present an updated taxonomy and characterization of existing methods. We also discuss mature and emerging application areas for LfD and highlight the significant challenges that remain to be overcome both in theory and in practice.}, number={1}, journal={Annual Review of Control, Robotics, and Autonomous Systems}, author={Ravichandar, Harish and Polydoros, Athanasios S. and Chernova, Sonia and Billard, Aude}, year={2020}, pages={297–330} }

 @article{Sukkar_Moreno_Vidal_Calleja_Deuse_2023, title={Guided Learning from Demonstration for Robust Transferability}, url={http://arxiv.org/abs/2302.03901}, abstractNote={Learning from demonstration (LfD) has the potential to greatly increase the applicability of robotic manipulators in modern industrial applications. Recent progress in LfD methods have put more emphasis in learning robustness than in guiding the demonstration itself in order to improve robustness. The latter is particularly important to consider when the target system reproducing the motion is structurally different to the demonstration system, as some demonstrated motions may not be reproducible. In light of this, this paper introduces a new guided learning from demonstration paradigm where an interactive graphical user interface (GUI) guides the user during demonstration, preventing them from demonstrating non-reproducible motions. The key aspect of our approach is determining the space of reproducible motions based on a motion planning framework which finds regions in the task space where trajectories are guaranteed to be of bounded length. We evaluate our method on two different setups with a six-degree-of-freedom (DOF) UR5 as the target system. First our method is validated using a seven-DOF Sawyer as the demonstration system. Then an extensive user study is carried out where several participants are asked to demonstrate, with and without guidance, a mock weld task using a hand held tool tracked by a VICON system. With guidance users were able to always carry out the task successfully in comparison to only 44% of the time without guidance.}, note={arXiv:2302.03901 [cs]}, number={arXiv:2302.03901}, publisher={arXiv}, author={Sukkar, Fouad and Moreno, Victor Hernandez and Vidal-Calleja, Teresa and Deuse, Jochen}, year={2023}, month=feb }

@article{sahin2020review,
  title={A review on object pose recovery: From 3D bounding box detectors to full 6D pose estimators},
  author={Sahin, Caner and Garcia-Hernando, Guillermo and Sock, Juil and Kim, Tae-Kyun},
  journal={Image and Vision Computing},
  volume={96},
  pages={103898},
  year={2020},
  publisher={Elsevier}
}

@article{firintepe2021ir,
  title={From IR images to point clouds to pose: point cloud-based AR glasses pose estimation},
  author={Firintepe, Ahmet and Vey, Carolin and Asteriadis, Stylianos and Pagani, Alain and Stricker, Didier},
  journal={Journal of Imaging},
  volume={7},
  number={5},
  pages={80},
  year={2021},
  publisher={MDPI}
}

% ----------- C3: Interactive generation of physics simulations from sensory data -----------

@inproceedings{mandlekar2018roboturk,
  title={Roboturk: A crowdsourcing platform for robotic skill learning through imitation},
  author={Mandlekar, Ajay and Zhu, Yuke and Garg, Animesh and Booher, Jonathan and Spero, Max and Tung, Albert and Gao, Julian and Emmons, John and Gupta, Anchit and Orbay, Emre and others},
  booktitle={Conference on Robot Learning},
  pages={879--893},
  year={2018},
  organization={PMLR}
}

@inproceedings{
Hu2020DiffTaichi:,
title={DiffTaichi: Differentiable Programming for Physical Simulation},
author={Yuanming Hu and Luke Anderson and Tzu-Mao Li and Qi Sun and Nathan Carr and Jonathan Ragan-Kelley and Fredo Durand},
booktitle={International Conference on Learning Representations},
year={2020},
url={https://openreview.net/forum?id=B1eB5xSFvr}
}

@inproceedings{
xu2022accelerated,
title={Accelerated Policy Learning with Parallel Differentiable Simulation},
author={Jie Xu and Miles Macklin and Viktor Makoviychuk and Yashraj Narang and Animesh Garg and Fabio Ramos and Wojciech Matusik},
booktitle={International Conference on Learning Representations},
year={2022},
url={https://openreview.net/forum?id=ZSKRQMvttc}
}

@inproceedings{
pfaff2021learning,
title={Learning Mesh-Based Simulation with Graph Networks},
author={Tobias Pfaff and Meire Fortunato and Alvaro Sanchez-Gonzalez and Peter Battaglia},
booktitle={International Conference on Learning Representations},
year={2021},
url={https://openreview.net/forum?id=roNqYL0_XP}
}

@article{allen2022inverse,
  title={Inverse Design for Fluid-Structure Interactions using Graph Network Simulators},
  author={Allen, Kelsey and Lopez-Guevara, Tatiana and Stachenfeld, Kimberly L and Sanchez Gonzalez, Alvaro and Battaglia, Peter and Hamrick, Jessica B and Pfaff, Tobias},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={13759--13774},
  year={2022}
}
@inproceedings{allen2023learning,
  title={Learning rigid dynamics with face interaction graph networks},
  author={Allen, Kelsey R and Rubanova, Yulia and Lopez-Guevara, Tatiana and Whitney, William F and Sanchez-Gonzalez, Alvaro and Battaglia, Peter and Pfaff, Tobias},
  booktitle={The Eleventh International Conference on Learning Representations},
  year={2023}
}

@inproceedings{longhini2023edo,
  title={Edo-net: Learning elastic properties of deformable objects from graph dynamics},
  author={Longhini, Alberta and Moletta, Marco and Reichlin, Alfredo and Welle, Michael C and Held, David and Erickson, Zackory and Kragic, Danica},
  booktitle={2023 IEEE International Conference on Robotics and Automation (ICRA)},
  pages={3875--3881},
  year={2023},
  organization={IEEE}
}



@inproceedings{sundaresan2022diffcloud, title={Diffcloud: Real-to-sim from point clouds with differentiable simulation and rendering of deformable objects},  author={Sundaresan, Priya and Antonova, Rika and Bohgl, Jeannette}, booktitle={2022 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)}, pages={10828--10835}, year={2022}, organization={IEEE} }

@article{antonova2022bayesian,
  title={A bayesian treatment of real-to-sim for deformable object manipulation},
  author={Antonova, Rika and Yang, Jingyun and Sundaresan, Priya and Fox, Dieter and Ramos, Fabio and Bohg, Jeannette},
  journal={IEEE Robotics and Automation Letters},
  volume={7},
  number={3},
  pages={5819--5826},
  year={2022},
  publisher={IEEE}
}
% papers on Real2Sim

@article{kandukuri2022physical,
  title={Physical representation learning and parameter identification from video using differentiable physics},
  author={Kandukuri, Rama Krishna and Achterhold, Jan and Moeller, Michael and Stueckler, Joerg},
  journal={International Journal of Computer Vision},
  pages={1--14},
  year={2022},
  publisher={Springer}
}

@misc{fang2023active,
      title={Active Task Randomization: Learning Robust Skills via Unsupervised Generation of Diverse and Feasible Tasks}, 
      author={Kuan Fang and Toki Migimatsu and Ajay Mandlekar and Li Fei-Fei and Jeannette Bohg},
      year={2023},
      eprint={2211.06134},
      archivePrefix={arXiv},
      primaryClass={cs.RO}
}

@inproceedings{wang2023gen,
author    = {Lirui Wang and Yiyang Ling and Zhecheng Yuan and Mohit Shridhar and Chen Bao and Yuzhe Qin and Bailin Wang and Huazhe Xu and Xiaolong Wang},
title     = {GenSim: Generating Robotic Simulation Tasks via Large Language Models},
booktitle = {Arxiv},
year      = {2023}
}

% ----------- C3: Interactive generation of physics simulations from sensory data -----------


% ----------- C4: Enhancing Manipulation Skill Learning for Scenes with Numerous Objects and Diverse
Geometries -----------
@article{shafiullah2022behavior,
  title={Behavior transformers: Cloning $ k $ modes with one stone},
  author={Shafiullah, Nur Muhammad and Cui, Zichen and Altanzaya, Ariuntuya Arty and Pinto, Lerrel},
  journal={Advances in neural information processing systems},
  volume={35},
  pages={22955--22968},
  year={2022}
}

@article{chi2023diffusion,
  title={Diffusion policy: Visuomotor policy learning via action diffusion},
  author={Chi, Cheng and Feng, Siyuan and Du, Yilun and Xu, Zhenjia and Cousineau, Eric and Burchfiel, Benjamin and Song, Shuran},
  journal={arXiv preprint arXiv:2303.04137},
  year={2023}
}


@article{pearce2023imitating,
  title={Imitating human behaviour with diffusion models},
  author={Pearce, Tim and Rashid, Tabish and Kanervisto, Anssi and Bignell, Dave and Sun, Mingfei and Georgescu, Raluca and Macua, Sergio Valcarcel and Tan, Shan Zheng and Momennejad, Ida and Hofmann, Katja and others},
  journal={arXiv preprint arXiv:2301.10677},
  year={2023}
}

@article{zhao2023learning,
  title={Learning fine-grained bimanual manipulation with low-cost hardware},
  author={Zhao, Tony Z and Kumar, Vikash and Levine, Sergey and Finn, Chelsea},
  journal={arXiv preprint arXiv:2304.13705},
  year={2023}
}

@inproceedings{zeng2018learning,
  title={Learning synergies between pushing and grasping with self-supervised deep reinforcement learning},
  author={Zeng, Andy and Song, Shuran and Welker, Stefan and Lee, Johnny and Rodriguez, Alberto and Funkhouser, Thomas},
  booktitle={2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
  pages={4238--4245},
  year={2018},
  organization={IEEE}
}

@article{zeng2020tossingbot,
  title={Tossingbot: Learning to throw arbitrary objects with residual physics},
  author={Zeng, Andy and Song, Shuran and Lee, Johnny and Rodriguez, Alberto and Funkhouser, Thomas},
  journal={IEEE Transactions on Robotics},
  volume={36},
  number={4},
  pages={1307--1319},
  year={2020},
  publisher={IEEE}
}

@inproceedings{zhu2023viola,
  title={VIOLA: Object-Centric Imitation Learning for Vision-Based Robot Manipulation},
  author={Zhu, Yifeng and Joshi, Abhishek and Stone, Peter and Zhu, Yuke},
  booktitle={Conference on Robot Learning},
  pages={1199--1210},
  year={2023},
  organization={PMLR}
}

@inproceedings{carvalho2022adapting,
  title={Adapting Object-Centric Probabilistic Movement Primitives with Residual Reinforcement Learning},
  author={Carvalho, Jo{\~a}o and Koert, Dorothea and Daniv, Marek and Peters, Jan},
  booktitle={2022 IEEE-RAS 21st International Conference on Humanoid Robots (Humanoids)},
  pages={405--412},
  year={2022},
  organization={IEEE}
}

@article{gao2023k,
  title={K-vil: Keypoints-based visual imitation learning},
  author={Gao, Jianfeng and Tao, Zhi and Jaquier, No{\'e}mie and Asfour, Tamim},
  journal={IEEE Transactions on Robotics},
  year={2023},
  publisher={IEEE}
}


% DMP 2006
@incollection{schaal2006dynamic,
  title={Dynamic movement primitives-a framework for motor control in humans and humanoid robotics},
  author={Schaal, Stefan},
  booktitle={Adaptive motion of animals and machines},
  pages={261--280},
  year={2006},
  publisher={Springer}
}


% DMP 2013
@article{ijspeert2013dynamical,
  title={Dynamical movement primitives: learning attractor models for motor behaviors},
  author={Ijspeert, Auke Jan and Nakanishi, Jun and Hoffmann, Heiko and Pastor, Peter and Schaal, Stefan},
  journal={Neural computation},
  volume={25},
  number={2},
  pages={328--373},
  year={2013},
  publisher={MIT Press One Rogers Street, Cambridge, MA 02142-1209, USA journals-info~…}
}



% MimicGen
@inproceedings{mandlekar2023mimicgen,
  title={MimicGen: A Data Generation System for Scalable Robot Learning using Human Demonstrations},
  author={Mandlekar, Ajay and Nasiriany, Soroush and Wen, Bowen and Akinola, Iretiayo and Narang, Yashraj and Fan, Linxi and Zhu, Yuke and Fox, Dieter},
  booktitle={7th Annual Conference on Robot Learning},
  year={2023}
}

% Via-point MP
@inproceedings{zhou2019learning,
  title={Learning via-point movement primitives with inter-and extrapolation capabilities},
  author={Zhou, You and Gao, Jianfeng and Asfour, Tamim},
  booktitle={2019 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
  pages={4301--4308},
  year={2019},
  organization={IEEE}
}


% ----------- C5: Data-Efficient Fine Tuning of Hierarchical Manipulation Skills  -----------
% RL fine tune
@INPROCEEDINGS{Zhu-RSS-18, 
    AUTHOR    = {Yuke Zhu AND Ziyu Wang AND Josh Merel AND Andrei Rusu AND Tom Erez AND Serkan Cabi AND Saran  Tunyasuvunakool AND JÃ¡nos KramÃ¡r AND Raia Hadsell AND Nando de Freitas AND Nicolas Heess}, 
    TITLE     = {Reinforcement and Imitation Learning for Diverse Visuomotor Skills}, 
    BOOKTITLE = {Proceedings of Robotics: Science and Systems}, 
    YEAR      = {2018}, 
    ADDRESS   = {Pittsburgh, Pennsylvania}, 
    MONTH     = {06}, 
    DOI       = {10.15607/RSS.2018.XIV.009} 
} 
  
% RL fine tune
@InProceedings{pmlr-v155-julian21a,
  title = 	 {Never Stop Learning: The Effectiveness of Fine-Tuning in Robotic Reinforcement Learning},
  author =       {Julian, Ryan and Swanson, Benjamin and Sukhatme, Gaurav and Levine, Sergey and Finn, Chelsea and Hausman, Karol},
  booktitle = 	 {Proceedings of the 2020 Conference on Robot Learning},
  pages = 	 {2120--2136},
  year = 	 {2021},
  editor = 	 {Kober, Jens and Ramos, Fabio and Tomlin, Claire},
  volume = 	 {155},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {11},
  publisher =    {PMLR},  
}

@inproceedings{ball2023efficient,
  title={Efficient online reinforcement learning with offline data},
  author={Ball, Philip J and Smith, Laura and Kostrikov, Ilya and Levine, Sergey},
  booktitle={International conference on machine learning},
  year={2023},
  organization={PMLR}
}


% SAC
@inproceedings{haarnoja2018soft,
  title={Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor},
  author={Haarnoja, Tuomas and Zhou, Aurick and Abbeel, Pieter and Levine, Sergey},
  booktitle={International conference on machine learning},
  pages={1861--1870},
  year={2018},
  organization={PMLR}
}

% DDPG
@article{lillicrap2015continuous,
  title={Continuous control with deep reinforcement learning},
  author={Lillicrap, Timothy P and Hunt, Jonathan J and Pritzel, Alexander and Heess, Nicolas and Erez, Tom and Tassa, Yuval and Silver, David and Wierstra, Daan},
  journal={arXiv preprint arXiv:1509.02971},
  year={2015}
}

% TD3 
@inproceedings{fujimoto2018addressing,
  title={Addressing function approximation error in actor-critic methods},
  author={Fujimoto, Scott and Hoof, Herke and Meger, David},
  booktitle={International conference on machine learning},
  pages={1587--1596},
  year={2018},
  organization={PMLR}
}

% IQL
@inproceedings{kostrikov2021offline,
  title={Offline Reinforcement Learning with Implicit Q-Learning},
  author={Kostrikov, Ilya and Nair, Ashvin and Levine, Sergey},
  booktitle={International Conference on Learning Representations},
  year={2021}
}

% Conservative Q-Learning
@inproceedings{kumar2020conservative,
  title={Conservative Q-learning for offline reinforcement learning},
  author={Kumar, Aviral and Zhou, Aurick and Tucker, George and Levine, Sergey},
  booktitle={Proceedings of the 34th International Conference on Neural Information Processing Systems},
  pages={1179--1191},
  year={2020}
}

% YCB
@ARTICLE{7254318,
  author={Calli, Berk and Walsman, Aaron and Singh, Arjun and Srinivasa, Siddhartha and Abbeel, Pieter and Dollar, Aaron M.},
  journal={IEEE Robotics & Automation Magazine}, 
  title={Benchmarking in Manipulation Research: Using the Yale-CMU-Berkeley Object and Model Set}, 
  year={2015},
  volume={22},
  number={3},
  pages={36-52},
  doi={10.1109/MRA.2015.2448951}}

% DLR, Guide RL using Shared Control Templates
@inproceedings{padalkar2023guiding,
  title={Guiding Reinforcement Learning with Shared Control Templates},
  author={Padalkar, Abhishek and Quere, Gabriel and Steinmetz, Franz and Raffin, Antonin and Nieuwenhuisen, Matthias and Silv{\'e}rio, Jo{\~a}o and Stulp, Freek},
  booktitle={40th IEEE International Conference on Robotics and Automation, ICRA 2023},
  year={2023},
  organization={IEEE}
}


@inproceedings{BalkcomM04,
  author       = {Devin J. Balkcom and
                  Matthew T. Mason},
  title        = {Introducing Robotic Origami Folding},
  booktitle    = {Proceedings of the 2004 {IEEE} International Conference on Robotics
                  and Automation, {ICRA} 2004, April 26 - May 1, 2004, New Orleans,
                  LA, {USA}},
  pages        = {3245--3250},
  publisher    = {{IEEE}},
  year         = {2004},
  url          = {https://doi.org/10.1109/ROBOT.2004.1308754},
  doi          = {10.1109/ROBOT.2004.1308754},
  timestamp    = {Mon, 22 May 2017 17:12:25 +0200},
  biburl       = {https://dblp.org/rec/conf/icra/BalkcomM04.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{ThaiSH18,
  author       = {Phuong Thao Thai and
                  Maria Savchenko and
                  Ichiro Hagiwara},
  title        = {Finite element simulation of robotic origami folding},
  journal      = {Simul. Model. Pract. Theory},
  volume       = {84},
  pages        = {251--267},
  year         = {2018},
  url          = {https://doi.org/10.1016/j.simpat.2018.03.004},
  doi          = {10.1016/J.SIMPAT.2018.03.004},
  timestamp    = {Thu, 14 Oct 2021 09:11:15 +0200},
  biburl       = {https://dblp.org/rec/journals/simpra/ThaiSH18.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{NamikiY15,
  author       = {Akio Namiki and
                  Shuichi Yokosawa},
  title        = {Robotic origami folding with dynamic motion primitives},
  booktitle    = {2015 {IEEE/RSJ} International Conference on Intelligent Robots and
                  Systems, {IROS} 2015, Hamburg, Germany, September 28 - October 2,
                  2015},
  pages        = {5623--5628},
  publisher    = {{IEEE}},
  year         = {2015},
  url          = {https://doi.org/10.1109/IROS.2015.7354175},
  doi          = {10.1109/IROS.2015.7354175},
  timestamp    = {Fri, 27 Dec 2019 21:21:56 +0100},
  biburl       = {https://dblp.org/rec/conf/iros/NamikiY15.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{ElbrechterHR12,
  author       = {Christof Elbrechter and
                  Robert Haschke and
                  Helge J. Ritter},
  title        = {Folding paper with anthropomorphic robot hands using real-time physics-based
                  modeling},
  booktitle    = {12th {IEEE-RAS} International Conference on Humanoid Robots (Humanoids
                  2012), Osaka, Japan, November 29 - Dec. 1, 2012},
  pages        = {210--215},
  publisher    = {{IEEE}},
  year         = {2012},
  url          = {https://doi.org/10.1109/HUMANOIDS.2012.6651522},
  doi          = {10.1109/HUMANOIDS.2012.6651522},
  timestamp    = {Sun, 25 Oct 2020 22:49:03 +0100},
  biburl       = {https://dblp.org/rec/conf/humanoids/ElbrechterHR12.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

% ----------- C6: Robust Pre-Learning in Simulation and Sim-to-Real Transfer -----------

@inproceedings{tobin2017domain,
  title={Domain randomization for transferring deep neural networks from simulation to the real world},
  author={Tobin, Josh and Fong, Rachel and Ray, Alex and Schneider, Jonas and Zaremba, Wojciech and Abbeel, Pieter},
  booktitle={2017 IEEE/RSJ international conference on intelligent robots and systems (IROS)},
  pages={23--30},
  year={2017},
  organization={IEEE}
}

@inproceedings{
clavera2018learning,
title={Learning to Adapt in Dynamic, Real-World Environments through Meta-Reinforcement Learning},
author={Ignasi Clavera and Anusha Nagabandi and Simin Liu and Ronald S. Fearing and Pieter Abbeel and Sergey Levine and Chelsea Finn},
booktitle={International Conference on Learning Representations},
year={2019},
url={https://openreview.net/forum?id=HyztsoC5Y7},
}


@InProceedings{Rao_2020_CVPR,
author = {Rao, Kanishka and Harris, Chris and Irpan, Alex and Levine, Sergey and Ibarz, Julian and Khansari, Mohi},
title = {RL-CycleGAN: Reinforcement Learning Aware Simulation-to-Real},
booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
month = {June},
year = {2020}
}

@inproceedings{song2020learning,
    title={Learning to Slide Unknown Objects with Differentiable Physics Simulations}, 
    author={Changkyu Song and Abdeslam Boularias},
    journal={Robotics: Science and Systems},
    year={2021}
}

@INPROCEEDINGS{zhao2020sim2real,
  author={Zhao, Wenshuai and Queralta, Jorge Peña and Westerlund, Tomi},
  booktitle={2020 IEEE Symposium Series on Computational Intelligence (SSCI)}, 
  title={Sim-to-Real Transfer in Deep Reinforcement Learning for Robotics: a Survey}, 
  year={2020},
  volume={},
  number={},
  pages={737-744},
  doi={10.1109/SSCI47803.2020.9308468}}


@inproceedings{kumar2021rma,
title={Rma: Rapid motor adaptation for legged robots},
author={Kumar, Ashish and Fu, Zipeng and Pathak, Deepak and Malik, Jitendra},
journal={Robotics: Science and Systems},
year={2021}

}

@inproceedings{
eysenbach2022maximum,
title={Maximum Entropy {RL} (Provably) Solves Some Robust {RL} Problems},
author={Benjamin Eysenbach and Sergey Levine},
booktitle={International Conference on Learning Representations},
year={2022},
url={https://openreview.net/forum?id=PtSAD3caaA2}
}




@misc{tiboni2023domain,
      title={Domain Randomization via Entropy Maximization}, 
      author={Gabriele Tiboni and Pascal Klink and Jan Peters and Tatiana Tommasi and Carlo D'Eramo and Georgia Chalvatzaki},
      year={2023},
      eprint={2311.01885},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@inproceedings{ge2023policy,
  title={Policy Adaptation From Foundation Model Feedback},
  author={Ge, Yuying and Macaluso, Annabella and Li, Li Erran and Luo, Ping and Wang, Xiaolong},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={19059--19069},
  year={2023}
}

% Push to See / grasp
@article{xu2021efficient,
  title={Efficient learning of goal-oriented push-grasping synergy in clutter},
  author={Xu, Kechun and Yu, Hongxiang and Lai, Qianen and Wang, Yue and Xiong, Rong},
  journal={IEEE Robotics and Automation Letters},
  volume={6},
  number={4},
  pages={6337--6344},
  year={2021},
  publisher={IEEE}
}


% Mechanical Search
@inproceedings{danielczuk2019mechanical,
  title={Mechanical search: Multi-step retrieval of a target object occluded by clutter},
  author={Danielczuk, Michael and Kurenkov, Andrey and Balakrishna, Ashwin and Matl, Matthew and Wang, David and Mart{\'\i}n-Mart{\'\i}n, Roberto and Garg, Animesh and Savarese, Silvio and Goldberg, Ken},
  booktitle={2019 International Conference on Robotics and Automation (ICRA)},
  pages={1614--1621},
  year={2019},
  organization={IEEE}
}

% Mechanical Search
@inproceedings{kurenkov2020visuomotor,
  title={Visuomotor mechanical search: Learning to retrieve target objects in clutter},
  author={Kurenkov, Andrey and Taglic, Joseph and Kulkarni, Rohun and Dominguez-Kuhne, Marcus and Garg, Animesh and Mart{\'\i}n-Mart{\'\i}n, Roberto and Savarese, Silvio},
  booktitle={2020 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
  pages={8408--8414},
  year={2020},
  organization={IEEE}
}

% Hierarchical IL + RL fine tuning
@article{gupta2019relay,
  title={Relay policy learning: Solving long-horizon tasks via imitation and reinforcement learning},
  author={Gupta, Abhishek and Kumar, Vikash and Lynch, Corey and Levine, Sergey and Hausman, Karol},
  journal={arXiv preprint arXiv:1910.11956},
  year={2019}
}

% Hierarchical RL in high level fine tuning
@article{li2019sub,
  title={Sub-policy adaptation for hierarchical reinforcement learning},
  author={Li, Alexander C and Florensa, Carlos and Clavera, Ignasi and Abbeel, Pieter},
  journal={arXiv preprint arXiv:1906.05862},
  year={2019}
}

% Latent exploration in Reinforcement Learning
@article{chiappa2023latent,
  title={Latent exploration for reinforcement learning},
  author={Chiappa, Alberto Silvio and Vargas, Alessandro Marin and Huang, Ann Zixiang and Mathis, Alexander},
  journal={Advances in Neural Information Processing Systems (NeurIPS)},
  year={2023}
}

@inproceedings{Ball2023rlpd,
  author       = {Philip J. Ball and
                  Laura M. Smith and
                  Ilya Kostrikov and
                  Sergey Levine},
  title        = {Efficient Online Reinforcement Learning with Offline Data},
  booktitle    = {International Conference on Machine Learning},
  year         = {2023},
}






% ----------- C6: Robust Pre-Learning in Simulation and Sim-to-Real Transfer -----------

% ----------- C7: Learning Generic and Transferable Reward Representations from Multiple Feedback Sources -----------


@InProceedings{brown2019drex,
  title = {Better-than-Demonstrator Imitation Learning via Automatically-Ranked Demonstrations},
  author = {Brown, Daniel S. and Goo, Wonjoon and Niekum, Scott},
  booktitle = {Proceedings of the 3rd Conference on Robot Learning},
  year = {2019}
}
@inproceedings{ChristianoNIPS2017,
 author = {Christiano, Paul F and Leike, Jan and Brown, Tom and Martic, Miljan and Legg, Shane and Amodei, Dario},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
 pages = {},
 title = {Deep Reinforcement Learning from Human Preferences},
 volume = {30},
 year = {2017}
}

@inproceedings{Garrett2018DeepTamer,
author = {Warnell, Garrett and Waytowich, Nicholas and Lawhern, Vernon and Stone, Peter},
title = {Deep TAMER: Interactive Agent Shaping in High-Dimensional State Spaces},
year = {2018},
isbn = {978-1-57735-800-8},
publisher = {AAAI Press},
booktitle = {Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence and Thirtieth Innovative Applications of Artificial Intelligence Conference and Eighth AAAI Symposium on Educational Advances in Artificial Intelligence},
articleno = {189},
numpages = {9},
location = {New Orleans, Louisiana, USA},
series = {AAAI'18/IAAI'18/EAAI'18}
}

@article{Losey2022PCorrections,
author = {Dylan P. Losey and Andrea Bajcsy and Marcia K. O’Malley and Anca D. Dragan},
title ={Physical interaction as communication: Learning robot objectives online from human corrections},
journal = {The International Journal of Robotics Research},
volume = {41},
number = {1},
pages = {20-44},
year = {2022},
}


@InProceedings{Biyik2018BatchPref,
  title = 	 {Batch Active Preference-Based Learning of Reward Functions},
  author =       {Biyik, Erdem and Sadigh, Dorsa},
  booktitle = 	 {Proceedings of The 2nd Conference on Robot Learning},
  pages = 	 {519--528},
  year = 	 {2018},
  editor = 	 {Billard, Aude and Dragan, Anca and Peters, Jan and Morimoto, Jun},
  volume = 	 {87},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {10},
  publisher =    {PMLR}
}

@inproceedings{
datta2023iifl,
title={{IIFL}: Implicit Interactive Fleet Learning from Heterogeneous Human Supervisors},
author={Gaurav Datta and Ryan Hoque and Anrui Gu and Eugen Solowjow and Ken Goldberg},
booktitle={7th Annual Conference on Robot Learning},
year={2023},
url={https://openreview.net/forum?id=VtUZ4VGPns}
}

@inproceedings{
hoque2022fleetdagger,
title={Fleet-{DA}gger: Interactive Robot Fleet Learning with Scalable Human Supervision},
author={Ryan Hoque and Lawrence Yunliang Chen and Satvik Sharma and Karthik Dharmarajan and Brijen Thananjeyan and Pieter Abbeel and Ken Goldberg},
booktitle={6th Annual Conference on Robot Learning},
year={2022},
url={https://openreview.net/forum?id=MoSC0pziRd}
}
@inproceedings{Pratyusha2020CorrectingNLF,
 author = {Sharma, Pratyusha and Sundaralingam, Balakumar and Blukis, Valts and Paxton, Chris and Hermans, Tucker and Torralba, Antonio and Andreas, Jacob and Fox, Dieter},
 booktitle = {Proceedings of Robotics: Science and Systems},
 title = {Correcting Robot Plans with Natural Language Feedback},
 year = {2022}
}


@ARTICLE{Jianfeng2023KVIL,
  author={Gao, Jianfeng and Tao, Zhi and Jaquier, Noémie and Asfour, Tamim},
  journal={IEEE Transactions on Robotics}, 
  title={K-VIL: Keypoints-Based Visual Imitation Learning}, 
  year={2023},
  volume={39},
  number={5},
  pages={3888-3908},
  doi={10.1109/TRO.2023.3286074}
}

@inproceedings{huang2023reparameterized,
  title={Reparameterized policy learning for multimodal trajectory optimization},
  author={Huang, Zhiao and Liang, Litian and Ling, Zhan and Li, Xuanlin and Gan, Chuang and Su, Hao},
  booktitle={International Conference on Machine Learning},
  pages={13957--13975},
  year={2023},
  organization={PMLR}
}


%------------ C9: Learning Long-Term Prediction Models of Object Interactions ----------

@article{shi2023robocook,
  title={RoboCook: Long-Horizon Elasto-Plastic Object Manipulation with Diverse Tools}, 
  author={Shi, Haochen and Xu, Huazhe and Clarke, Samuel and Li, Yunzhu and Wu, Jiajun},
  journal={arXiv preprint arXiv:2306.14447},
  year={2023},
}

@article{kalman1960,
    author = {Kalman, R. E.},
    title = "{A New Approach to Linear Filtering and Prediction Problems}",
    journal = {Journal of Basic Engineering},
    volume = {82},
    number = {1},
    pages = {35-45},
    year = {1960},
    month = {03},
    abstract = "{The classical filtering and prediction problem is re-examined using the Bode-Shannon representation of random processes and the “state-transition” method of analysis of dynamic systems. New results are: (1) The formulation and methods of solution of the problem apply without modification to stationary and nonstationary statistics and to growing-memory and infinite-memory filters. (2) A nonlinear difference (or differential) equation is derived for the covariance matrix of the optimal estimation error. From the solution of this equation the co-efficients of the difference (or differential) equation of the optimal linear filter are obtained without further calculations. (3) The filtering problem is shown to be the dual of the noise-free regulator problem. The new method developed here is applied to two well-known problems, confirming and extending earlier results. The discussion is largely self-contained and proceeds from first principles; basic concepts of the theory of random processes are reviewed in the Appendix.}",
    issn = {0021-9223},
    doi = {10.1115/1.3662552},
    url = {https://doi.org/10.1115/1.3662552},
    eprint = {https://asmedigitalcollection.asme.org/fluidsengineering/article-pdf/82/1/35/5518977/35\_1.pdf},
}

@inproceedings{liu2022Transformer,
  author       = {Yong Liu and
                  Haixu Wu and
                  Jianmin Wang and
                  Mingsheng Long},
  title        = {Non-stationary Transformers: Exploring the Stationarity in Time Series
                  Forecasting},
  booktitle    = {NeurIPS},
  year         = {2022},
  url          = {http://papers.nips.cc/paper\_files/paper/2022/hash/4054556fcaa934b0bf76da52cf4f92cb-Abstract-Conference.html},
  timestamp    = {Thu, 29 Jun 2023 16:58:02 +0200},
  biburl       = {https://dblp.org/rec/conf/nips/LiuWWL22.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}


@inproceedings{vaswani2017attention,
  author       = {Ashish Vaswani and
                  Noam Shazeer and
                  Niki Parmar and
                  Jakob Uszkoreit and
                  Llion Jones and
                  Aidan N. Gomez and
                  Lukasz Kaiser and
                  Illia Polosukhin},
  editor       = {Isabelle Guyon and
                  Ulrike von Luxburg and
                  Samy Bengio and
                  Hanna M. Wallach and
                  Rob Fergus and
                  S. V. N. Vishwanathan and
                  Roman Garnett},
  title        = {Attention is All you Need},
  booktitle    = {Advances in Neural Information Processing Systems 30: Annual Conference
                  on Neural Information Processing Systems 2017, December 4-9, 2017,
                  Long Beach, CA, {USA}},
  pages        = {5998--6008},
  year         = {2017},
  url          = {https://proceedings.neurips.cc/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html},
  timestamp    = {Thu, 21 Jan 2021 15:15:21 +0100},
  biburl       = {https://dblp.org/rec/conf/nips/VaswaniSPUJGKP17.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

%------------ C9 [new]: Fine tuning foundational representations to specific tasks ----------
@inproceedings{liu2022herd,
  title={HERD: Continuous Human-to-Robot Evolution for Learning from Human Demonstration},
  author={Liu, Xingyu and Pathak, Deepak and Kitani, Kris M},
  booktitle={6th Annual Conference on Robot Learning},
  year={2022}
}
%% Morphs a human hand into a simple actuator to simplify the action space

@article{chen2023development,
  title={Development of a Three-Fingered Multi-Modality Dexterous Hand with Integrated Embedded High-Dimensional Sensors},
  author={Chen, Mingqi and Li, Shaodong and Shuang, Feng and Du, Yang and Liu, Xi},
  journal={Journal of Intelligent \& Robotic Systems},
  volume={108},
  number={2},
  pages={24},
  year={2023},
  publisher={Springer}
}
@inproceedings{saloutos2023towards,
  title={Towards robust autonomous grasping with reflexes using high-bandwidth sensing and actuation},
  author={SaLoutos, Andrew and Kim, Hongmin and Stanger-Jones, Elijah and Guo, Menglong and Kim, Sangbae},
  booktitle={2023 IEEE International Conference on Robotics and Automation (ICRA)},
  pages={10254--10260},
  year={2023},
  organization={IEEE}
}
@article{cairnes2023overview,
  title={An overview of robotic grippers},
  author={Cairnes, Thomas J and Ford, Christopher J and Psomopoulou, Efi and Lepora, Nathan},
  journal={IEEE Potentials},
  volume={42},
  number={3},
  pages={17--23},
  year={2023},
  publisher={IEEE}
}
%% Simple human hands and other simple grippers

@inproceedings{mandlekar2021matters,
  title={What Matters in Learning from Offline Human Demonstrations for Robot Manipulation},
  author={Mandlekar, Ajay and Xu, Danfei and Wong, Josiah and Nasiriany, Soroush and Wang, Chen and Kulkarni, Rohun and Fei-Fei, Li and Savarese, Silvio and Zhu, Yuke and Mart{\'\i}n-Mart{\'\i}n, Roberto},
  booktitle={5th Annual Conference on Robot Learning},
  year={2021}
}
% challenges in learning from human demonstrations

@article{negrello2020hands,
  title={Hands in the real world},
  author={Negrello, Francesca and Stuart, Hannah S and Catalano, Manuel G},
  journal={Frontiers in Robotics and AI},
  volume={6},
  pages={147},
  year={2020},
  publisher={Frontiers Media SA}
}


@article{kadalagere2023Review,
author = {Kadalagere Sampath, Suhas and Wang, Ning and Wu, Hao and Yang, Chenguang},
title = {Review on human-like robot manipulation using dexterous hands},
journal = {Cognitive Computation and Systems},
volume = {5},
number = {1},
pages = {14-29},
keywords = {dexterous hand, learning-based manipulation, robot manipulation},
doi = {https://doi.org/10.1049/ccs2.12073},
url = {https://ietresearch.onlinelibrary.wiley.com/doi/abs/10.1049/ccs2.12073},
eprint = {https://ietresearch.onlinelibrary.wiley.com/doi/pdf/10.1049/ccs2.12073},
year = {2023}
}

@ARTICLE{Li2022Survey,
AUTHOR={Li, Yinlin and Wang, Peng and Li, Rui and Tao, Mo and Liu, Zhiyong and Qiao, Hong},   
TITLE={A Survey of Multifingered Robotic Manipulation: Biological Results, Structural Evolvements, and Learning Methods},      
JOURNAL={Frontiers in Neurorobotics},      
VOLUME={16},           
YEAR={2022},      
URL={https://www.frontiersin.org/articles/10.3389/fnbot.2022.843267},       
DOI={10.3389/fnbot.2022.843267},      
ISSN={1662-5218},   
}

%% hands are well-suited for complex tasks

@article{navarro2023visuo,
  title={Visuo-haptic object perception for robots: an overview},
  author={Navarro-Guerrero, Nicol{\'a}s and Toprak, Sibel and Josifovski, Josip and Jamone, Lorenzo},
  journal={Autonomous Robots},
  volume={47},
  number={4},
  pages={377--403},
  year={2023},
  publisher={Springer}
}

%% current sota on visuo-haptic perception and their sensor fusion in robotics

@article{lambeta2020digit,
  title={Digit: A novel design for a low-cost compact high-resolution tactile sensor with application to in-hand manipulation},
  author={Lambeta, Mike and Chou, Po-Wei and Tian, Stephen and Yang, Brian and Maloon, Benjamin and Most, Victoria Rose and Stroud, Dave and Santos, Raymond and Byagowi, Ahmad and Kammerer, Gregg and others},
  journal={IEEE Robotics and Automation Letters},
  volume={5},
  number={3},
  pages={3838--3845},
  year={2020},
  publisher={IEEE}
}
@article{ward2018tactip,
  title={The tactip family: Soft optical tactile sensors with 3d-printed biomimetic morphologies},
  author={Ward-Cherrier, Benjamin and Pestell, Nicholas and Cramphorn, Luke and Winstone, Benjamin and Giannaccini, Maria Elena and Rossiter, Jonathan and Lepora, Nathan F},
  journal={Soft robotics},
  volume={5},
  number={2},
  pages={216--227},
  year={2018},
  publisher={Mary Ann Liebert, Inc. 140 Huguenot Street, 3rd Floor New Rochelle, NY 10801 USA}
}
@article{pan2022task,
  title={Task-Driven In-Hand Manipulation of Unknown Objects with Tactile Sensing},
  author={Pan, Chaoyi and Lepert, Marion and Yuan, Shenli and Antonova, Rika and Bohg, Jeannette},
  journal={arXiv preprint arXiv:2210.13403},
  year={2022}
}
%% papers that use touch

%% Low dimensional grasp spaces
@inproceedings{starke2018synergy,
  title={Synergy-based, data-driven generation of object-specific grasps for anthropomorphic hands},
  author={Starke, Julia and Eichmann, Christian and Ottenhaus, Simon and Asfour, Tamim},
  booktitle={2018 IEEE-RAS 18th International Conference on Humanoid Robots (Humanoids)},
  pages={327--333},
  year={2018},
  organization={IEEE}
}
@INPROCEEDINGS{starke2021tempsynergy,
  author={Starke, Julia and Keller, Marco and Asfour, Amim},
  booktitle={2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)}, 
  title={Temporal Force Synergies in Human Grasping}, 
  year={2021},
  volume={},
  number={},
  pages={3963-3970},
  doi={10.1109/IROS51168.2021.9636223}
}

@Article{rivera2021synergy,
AUTHOR = {Rivera, Patricio and Valarezo Añazco, Edwin and Kim, Tae-Seong},
TITLE = {Object Manipulation with an Anthropomorphic Robotic Hand via Deep Reinforcement Learning with a Synergy Space of Natural Hand Poses},
JOURNAL = {Sensors},
VOLUME = {21},
YEAR = {2021},
NUMBER = {16},
ARTICLE-NUMBER = {5301},
URL = {https://www.mdpi.com/1424-8220/21/16/5301},
PubMedID = {34450741},
ISSN = {1424-8220},
DOI = {10.3390/s21165301}
}


%------------ C10: Fine tuning foundational representations to specific tasks ----------

@misc{yang2023foundation,
      title={Foundation Models for Decision Making: Problems, Methods, and Opportunities}, 
      author={Sherry Yang and Ofir Nachum and Yilun Du and Jason Wei and Pieter Abbeel and Dale Schuurmans},
      year={2023},
      eprint={2303.04129},
      archivePrefix={arXiv},
      primaryClass={cs.AI}
}


@misc{bommasani2022opportunities,
      title={On the Opportunities and Risks of Foundation Models}, 
      author={Rishi Bommasani and Drew A. Hudson and Ehsan Adeli and Russ Altman and Simran Arora and Sydney von Arx and Michael S. Bernstein and Jeannette Bohg and Antoine Bosselut and Emma Brunskill and Erik Brynjolfsson and Shyamal Buch and Dallas Card and Rodrigo Castellon and Niladri Chatterji and Annie Chen and Kathleen Creel and Jared Quincy Davis and Dora Demszky and Chris Donahue and Moussa Doumbouya and Esin Durmus and Stefano Ermon and John Etchemendy and Kawin Ethayarajh and Li Fei-Fei and Chelsea Finn and Trevor Gale and Lauren Gillespie and Karan Goel and Noah Goodman and Shelby Grossman and Neel Guha and Tatsunori Hashimoto and Peter Henderson and John Hewitt and Daniel E. Ho and Jenny Hong and Kyle Hsu and Jing Huang and Thomas Icard and Saahil Jain and Dan Jurafsky and Pratyusha Kalluri and Siddharth Karamcheti and Geoff Keeling and Fereshte Khani and Omar Khattab and Pang Wei Koh and Mark Krass and Ranjay Krishna and Rohith Kuditipudi and Ananya Kumar and Faisal Ladhak and Mina Lee and Tony Lee and Jure Leskovec and Isabelle Levent and Xiang Lisa Li and Xuechen Li and Tengyu Ma and Ali Malik and Christopher D. Manning and Suvir Mirchandani and Eric Mitchell and Zanele Munyikwa and Suraj Nair and Avanika Narayan and Deepak Narayanan and Ben Newman and Allen Nie and Juan Carlos Niebles and Hamed Nilforoshan and Julian Nyarko and Giray Ogut and Laurel Orr and Isabel Papadimitriou and Joon Sung Park and Chris Piech and Eva Portelance and Christopher Potts and Aditi Raghunathan and Rob Reich and Hongyu Ren and Frieda Rong and Yusuf Roohani and Camilo Ruiz and Jack Ryan and Christopher Ré and Dorsa Sadigh and Shiori Sagawa and Keshav Santhanam and Andy Shih and Krishnan Srinivasan and Alex Tamkin and Rohan Taori and Armin W. Thomas and Florian Tramèr and Rose E. Wang and William Wang and Bohan Wu and Jiajun Wu and Yuhuai Wu and Sang Michael Xie and Michihiro Yasunaga and Jiaxuan You and Matei Zaharia and Michael Zhang and Tianyi Zhang and Xikun Zhang and Yuhui Zhang and Lucia Zheng and Kaitlyn Zhou and Percy Liang},
      year={2022},
      eprint={2108.07258},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@inproceedings{rt12022arxiv,
    title={RT-1: Robotics Transformer for Real-World Control at Scale},
    author={Anthony	Brohan and  Noah Brown and  Justice Carbajal and  Yevgen Chebotar and  Joseph Dabis and  Chelsea Finn and  Keerthana Gopalakrishnan and  Karol Hausman and  Alex Herzog and  Jasmine Hsu and  Julian Ibarz and  Brian Ichter and  Alex Irpan and  Tomas Jackson and  Sally Jesmonth and  Nikhil Joshi and  Ryan Julian and  Dmitry Kalashnikov and  Yuheng Kuang and  Isabel Leal and  Kuang-Huei Lee and  Sergey Levine and  Yao Lu and  Utsav Malla and  Deeksha Manjunath and  Igor Mordatch and  Ofir Nachum and  Carolina Parada and  Jodilyn Peralta and  Emily Perez and  Karl Pertsch and  Jornell Quiambao and  Kanishka Rao and  Michael Ryoo and  Grecia Salazar and  Pannag Sanketi and  Kevin Sayed and  Jaspiar Singh and  Sumedh Sontakke and  Austin Stone and  Clayton Tan and  Huong Tran and  Vincent Vanhoucke and Steve Vega and  Quan Vuong and  Fei Xia and  Ted Xiao and  Peng Xu and  Sichun Xu and  Tianhe Yu and  Brianna Zitkovich},
    booktitle={arXiv preprint arXiv:2212.06817},
    year={2022}
}
@inproceedings{rt22023arxiv,
    title={RT-2: Vision-Language-Action Models Transfer Web Knowledge to Robotic Control},
    author={Anthony Brohan and Noah Brown and Justice Carbajal and Yevgen Chebotar and Xi Chen and Krzysztof Choromanski and Tianli Ding and Danny Driess and Avinava Dubey and Chelsea Finn and Pete Florence and Chuyuan Fu and Montse Gonzalez Arenas and Keerthana Gopalakrishnan and Kehang Han and Karol Hausman and Alex Herzog and Jasmine Hsu and Brian Ichter and Alex Irpan and Nikhil Joshi and Ryan Julian and Dmitry Kalashnikov and Yuheng Kuang and Isabel Leal  and Lisa Lee and Tsang-Wei Edward Lee and Sergey Levine and Yao Lu and Henryk Michalewski and Igor Mordatch and Karl Pertsch and Kanishka Rao and Krista Reymann and Michael Ryoo and Grecia Salazar and Pannag Sanketi and Pierre Sermanet and Jaspiar Singh and Anikait Singh and Radu Soricut and Huong Tran and Vincent Vanhoucke and Quan Vuong and Ayzaan Wahid and Stefan Welker and Paul Wohlhart and  Jialin Wu and Fei Xia and Ted Xiao and Peng Xu and Sichun Xu and Tianhe Yu and Brianna Zitkovich},
    booktitle={arXiv preprint arXiv:2307.15818},
    year={2023}
}

@misc{rtx2023arxiv,
title={Open {X-E}mbodiment: Robotic Learning Datasets and {RT-X} Models},
author = {Open X-Embodiment Collaboration and Abhishek Padalkar and Acorn Pooley and Ajinkya Jain and Alex Bewley and Alex Herzog and Alex Irpan and Alexander Khazatsky and Anant Rai and Anikait Singh and Anthony Brohan and Antonin Raffin and Ayzaan Wahid and Ben Burgess-Limerick and Beomjoon Kim and Bernhard Schölkopf and Brian Ichter and Cewu Lu and Charles Xu and Chelsea Finn and Chenfeng Xu and Cheng Chi and Chenguang Huang and Christine Chan and Chuer Pan and Chuyuan Fu and Coline Devin and Danny Driess and Deepak Pathak and Dhruv Shah and Dieter Büchler and Dmitry Kalashnikov and Dorsa Sadigh and Edward Johns and Federico Ceola and Fei Xia and Freek Stulp and Gaoyue Zhou and Gaurav S. Sukhatme and Gautam Salhotra and Ge Yan and Giulio Schiavi and Hao Su and Hao-Shu Fang and Haochen Shi and Heni Ben Amor and Henrik I Christensen and Hiroki Furuta and Homer Walke and Hongjie Fang and Igor Mordatch and Ilija Radosavovic and Isabel Leal and Jacky Liang and Jaehyung Kim and Jan Schneider and Jasmine Hsu and Jeannette Bohg and Jeffrey Bingham and Jiajun Wu and Jialin Wu and Jianlan Luo and Jiayuan Gu and Jie Tan and Jihoon Oh and Jitendra Malik and Jonathan Tompson and Jonathan Yang and Joseph J. Lim and João Silvério and Junhyek Han and Kanishka Rao and Karl Pertsch and Karol Hausman and Keegan Go and Keerthana Gopalakrishnan and Ken Goldberg and Kendra Byrne and Kenneth Oslund and Kento Kawaharazuka and Kevin Zhang and Keyvan Majd and Krishan Rana and Krishnan Srinivasan and Lawrence Yunliang Chen and Lerrel Pinto and Liam Tan and Lionel Ott and Lisa Lee and Masayoshi Tomizuka and Maximilian Du and Michael Ahn and Mingtong Zhang and Mingyu Ding and Mohan Kumar Srirama and Mohit Sharma and Moo Jin Kim and Naoaki Kanazawa and Nicklas Hansen and Nicolas Heess and Nikhil J Joshi and Niko Suenderhauf and Norman Di Palo and Nur Muhammad Mahi Shafiullah and Oier Mees and Oliver Kroemer and Pannag R Sanketi and Paul Wohlhart and Peng Xu and Pierre Sermanet and Priya Sundaresan and Quan Vuong and Rafael Rafailov and Ran Tian and Ria Doshi and Roberto Martín-Martín and Russell Mendonca and Rutav Shah and Ryan Hoque and Ryan Julian and Samuel Bustamante and Sean Kirmani and Sergey Levine and Sherry Moore and Shikhar Bahl and Shivin Dass and Shuran Song and Sichun Xu and Siddhant Haldar and Simeon Adebola and Simon Guist and Soroush Nasiriany and Stefan Schaal and Stefan Welker and Stephen Tian and Sudeep Dasari and Suneel Belkhale and Takayuki Osa and Tatsuya Harada and Tatsuya Matsushima and Ted Xiao and Tianhe Yu and Tianli Ding and Todor Davchev and Tony Z. Zhao and Travis Armstrong and Trevor Darrell and Vidhi Jain and Vincent Vanhoucke and Wei Zhan and Wenxuan Zhou and Wolfram Burgard and Xi Chen and Xiaolong Wang and Xinghao Zhu and Xuanlin Li and Yao Lu and Yevgen Chebotar and Yifan Zhou and Yifeng Zhu and Ying Xu and Yixuan Wang and Yonatan Bisk and Yoonyoung Cho and Youngwoon Lee and Yuchen Cui and Yueh-hua Wu and Yujin Tang and Yuke Zhu and Yunzhu Li and Yusuke Iwasawa and Yutaka Matsuo and Zhuo Xu and Zichen Jeff Cui},
howpublished  = {\url{https://arxiv.org/abs/2310.08864}},
year = {2023},
}
@article{kirillov2023segany,
  title={Segment Anything},
  author={Kirillov, Alexander and Mintun, Eric and Ravi, Nikhila and Mao, Hanzi and Rolland, Chloe and Gustafson, Laura and Xiao, Tete and Whitehead, Spencer and Berg, Alexander C. and Lo, Wan-Yen and Doll{\'a}r, Piotr and Girshick, Ross},
  journal={arXiv:2304.02643},
  year={2023}
}


 @inproceedings{goodfellow2014gan,
  title={Generative adversarial nets},
  author={Goodfellow, Ian and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
  booktitle={Advances in neural information processing systems},
  pages={2672--2680},
  year={2014}
} 

@inproceedings{saycan2022arxiv,
    title={Do As I Can and Not As I Say: Grounding Language in Robotic Affordances},
    author={Michael Ahn and Anthony Brohan and Noah Brown and Yevgen Chebotar and Omar Cortes and Byron David and Chelsea Finn and Chuyuan Fu and Keerthana Gopalakrishnan and Karol Hausman and Alex Herzog and Daniel Ho and Jasmine Hsu and Julian Ibarz and Brian Ichter and Alex Irpan and Eric Jang and Rosario Jauregui Ruano and Kyle Jeffrey and Sally Jesmonth and Nikhil Joshi and Ryan Julian and Dmitry Kalashnikov and Yuheng Kuang and Kuang-Huei Lee and Sergey Levine and Yao Lu and Linda Luu and Carolina Parada and Peter Pastor and Jornell Quiambao and Kanishka Rao and Jarek Rettinghouse and Diego Reyes and Pierre Sermanet and Nicolas Sievers and Clayton Tan and Alexander Toshev and Vincent Vanhoucke and Fei Xia and Ted Xiao and Peng Xu and Sichun Xu and Mengyuan Yan and Andy Zeng},
    booktitle={arXiv preprint arXiv:2204.01691},
    year={2022}
}

@inproceedings{Ouyang2022InstructGPT,
 author = {Ouyang, Long and Wu, Jeffrey and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and Schulman, John and Hilton, Jacob and Kelton, Fraser and Miller, Luke and Simens, Maddie and Askell, Amanda and Welinder, Peter and Christiano, Paul F and Leike, Jan and Lowe, Ryan},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Koyejo and S. Mohamed and A. Agarwal and D. Belgrave and K. Cho and A. Oh},
 pages = {27730--27744},
 publisher = {Curran Associates, Inc.},
 title = {Training language models to follow instructions with human feedback},
 url = {https://proceedings.neurips.cc/paper_files/paper/2022/file/b1efde53be364a73914f58805a001731-Paper-Conference.pdf},
 volume = {35},
 year = {2022}
}

@inproceedings{wu2023read,
  title = {Read and Reap the Rewards: Learning to Play Atari with the Help of Instruction Manuals},
  author = {Wu, Yue and Fan, Yewen and Liang, Paul Pu and Azaria, Amos and Li, Yuanzhi and Mitchell, Tom M},
  booktitle = {NeurIPS},
  year = {2023},
}

@misc{Sundaresan2023RTSketch,
  author = {Sundaresan, Priya and Vuong, Quan and Gu, Jiayuan and Xu, Peng and Xiao, Ted and Kirmani, Sean and Yu, Tianhe and Stark, Michael and Jain, Ajinkya and Hausman, Karol and Sadigh, Dorsa and Bohg, Jeannette and Schaal, Stefan},
  title = {{RT-Sketch: Goal-Conditioned Imitation Learning from Hand-Drawn Sketches}},
  year = {2023},
  url = {https://rt-sketch.github.io/},
  note = {Accessed: [date of access]}
}

@inproceedings{
yu2023language,
title={Language to Rewards for Robotic Skill Synthesis},
author={Wenhao Yu and Nimrod Gileadi and Chuyuan Fu and Sean Kirmani and Kuang-Huei Lee and Montserrat Gonzalez Arenas and Hao-Tien Lewis Chiang and Tom Erez and Leonard Hasenclever and Jan Humplik and brian ichter and Ted Xiao and Peng Xu and Andy Zeng and Tingnan Zhang and Nicolas Heess and Dorsa Sadigh and Jie Tan and Yuval Tassa and Fei Xia},
booktitle={7th Annual Conference on Robot Learning},
year={2023},
url={https://openreview.net/forum?id=SgTPdyehXMA}
}

@inproceedings{
nair2021learning,
title={Learning Language-Conditioned Robot Behavior from Offline Data and Crowd-Sourced Annotation},
author={Suraj Nair and Eric Mitchell and Kevin Chen and brian ichter and Silvio Savarese and Chelsea Finn},
booktitle={5th Annual Conference on Robot Learning },
year={2021},
}


@inproceedings{wang2022translating,
  author = {Ruocheng Wang and Yunzhi Zhang and Jiayuan Mao and Chin-Yi Cheng and Jiajun Wu},
  title = {Translating a Visual LEGO Manual to a Machine-Executable Plan},
  booktitle = {European Conference on Computer Vision},
  year={2022}
}

@INPROCEEDINGS{Rajeswaran2018Dexterous,
    author    = {Aravind Rajeswaran AND Vikash Kumar AND Abhishek Gupta AND
                 Giulia Vezzani AND John Schulman AND Emanuel Todorov AND Sergey Levine},
    title     = "{Learning Complex Dexterous Manipulation with Deep Reinforcement Learning and Demonstrations}",
    booktitle = {Proceedings of Robotics: Science and Systems (RSS)},
    year      = {2018},
}

%------------ WP1: Enhancing Intuitiveness and Scalability in Robot Teaching ----------

Isaac Sim/Orbit
@article{mittal2023orbit,
  title={Orbit: A Unified Simulation Framework for Interactive Robot Learning Environments}, 
  author={Mittal, Mayank and Yu, Calvin and Yu, Qinxi and Liu, Jingzhou and Rudin, Nikita and Hoeller, David and Yuan, Jia Lin and Singh, Ritvik and Guo, Yunrong and Mazhar, Hammad and Mandlekar, Ajay and Babich, Buck and State, Gavriel and Hutter, Marco and Garg, Animesh},
  journal={IEEE Robotics and Automation Letters}, 
  year={2023},
  volume={8},
  number={6},
  pages={3740-3747},
  doi={10.1109/LRA.2023.3270034}
}

%  Trossen Robotic
@misc{trossenroboticsTrossenRobotics,
	author = {},
	title = {{T}rossen {R}obotics - {R}obotic {A}rms, {C}rawlers {T}urrets and more! --- trossenrobotics.com},
	howpublished = {\url{https://www.trossenrobotics.com/}},
	year = {},
	note = {[Accessed 28-11-2023]},
}

% shadowrobot
@misc{shadowrobotShadowRobot,
	author = {},
	title = {{S}hadow {R}obot | {D}exterous {R}obotic {H}ands \& {T}eleoperated {R}obots --- shadowrobot.com},
	howpublished = {\url{https://www.shadowrobot.com/}},
	year = {},
	note = {[Accessed 28-11-2023]},
}

%------------ WP3: Scaling Up Manipulation Skill Learning ----------
% The first GNN paper
@article{scarselli2008graph,
  title={The graph neural network model},
  author={Scarselli, Franco and Gori, Marco and Tsoi, Ah Chung and Hagenbuchner, Markus and Monfardini, Gabriele},
  journal={IEEE transactions on neural networks},
  volume={20},
  number={1},
  pages={61--80},
  year={2008},
  publisher={IEEE}
}

% Point Transformer
@inproceedings{zhao2021point,
  title={Point transformer},
  author={Zhao, Hengshuang and Jiang, Li and Jia, Jiaya and Torr, Philip HS and Koltun, Vladlen},
  booktitle={Proceedings of the IEEE/CVF international conference on computer vision},
  pages={16259--16268},
  year={2021}
}

% NASA-TLX
@inproceedings{hart2006nasa,
  title={NASA-task load index (NASA-TLX); 20 years later},
  author={Hart, Sandra G},
  booktitle={Proceedings of the human factors and ergonomics society annual meeting},
  volume={50},
  number={9},
  pages={904--908},
  year={2006},
  organization={Sage publications Sage CA: Los Angeles, CA}
}
% Likert scales
@article{allen2007likert,
  title={Likert scales and data analyses},
  author={Allen, I Elaine and Seaman, Christopher A},
  journal={Quality progress},
  volume={40},
  number={7},
  pages={64--65},
  year={2007}
}

%------------ WP2: Representations Few Shot Learning----------

% Put BibTeX entries for your own work in this file
%voltron 
@inproceedings{karamcheti2023voltron,
  title={Language-Driven Representation Learning for Robotics},
  author={Siddharth Karamcheti and Suraj Nair and Annie S. Chen and Thomas Kollar and Chelsea Finn and Dorsa Sadigh and Percy Liang},
  booktitle={Robotics: Science and Systems (RSS)},
  year={2023}
}
%mvp
@article{Radosavovic2022mvp,
  title = {Real-World Robot Learning with Masked Visual Pre-training},
  author = {Ilija Radosavovic and Tete Xiao and Stephen James and Pieter Abbeel and Jitendra Malik and Trevor Darrell},
  year = {2022},
  journal = {CoRL}
}

@inproceedings{
liu2022masked,
title={Masked Autoencoding for Scalable and Generalizable Decision Making},
author={Fangchen Liu and Hao Liu and Aditya Grover and Pieter Abbeel},
booktitle={Advances in Neural Information Processing Systems},
editor={Alice H. Oh and Alekh Agarwal and Danielle Belgrave and Kyunghyun Cho},
year={2022},
url={https://openreview.net/forum?id=lNokkSaUbfV}
}

@InProceedings{garnelo12018np,
  title = 	 {Conditional Neural Processes},
  author =       {Garnelo, Marta and Rosenbaum, Dan and Maddison, Christopher and Ramalho, Tiago and Saxton, David and Shanahan, Murray and Teh, Yee Whye and Rezende, Danilo and Eslami, S. M. Ali},
  booktitle = 	 {Proceedings of the 35th International Conference on Machine Learning},
  pages = 	 {1704--1713},
  year = 	 {2018},
  editor = 	 {Dy, Jennifer and Krause, Andreas},
  volume = 	 {80},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {7},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v80/garnelo18a/garnelo18a.pdf},
  url = 	 {https://proceedings.mlr.press/v80/garnelo18a.html},
}


% Risks

@misc{zivid,
  title = {Zivid},
  howpublished = {\url{https://www.zivid.com/}},
  note = {Accessed: \today},
}

@misc{abaqus,
  title = {Abaqus by Dassault Systèmes},
  howpublished = {\url{https://www.3ds.com/products-services/simulia/products/abaqus/}},
  note = {Accessed: \today},
}

@inproceedings{NEURIPS2021_376c6b9f,
 author = {Stanton, Samuel and Izmailov, Pavel and Kirichenko, Polina and Alemi, Alexander A and Wilson, Andrew G},
 booktitle = {Advances in Neural Information Processing Systems},
 title = {Does Knowledge Distillation Really Work?},
 volume = {34},
 year = {2021}
}


@misc{shap-e,
  title = {Shap-E},
  howpublished = {\url{https://github.com/openai/shap-e}},
  note = {Accessed: \today},
}

@inproceedings{
hu2022lora,
title={Lo{RA}: Low-Rank Adaptation of Large Language Models},
author={Edward J Hu and yelong shen and Phillip Wallis and Zeyuan Allen-Zhu and Yuanzhi Li and Shean Wang and Lu Wang and Weizhu Chen},
booktitle={International Conference on Learning Representations},
year={2022},
url={https://openreview.net/forum?id=nZeVKeeFYf9}
}

@article{wilson2020bayesian,
  title={Bayesian deep learning and a probabilistic perspective of generalization},
  author={Wilson, Andrew G and Izmailov, Pavel},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={4697--4708},
  year={2020}
}

@article{Aravind2018,
  title={Learning Complex Dexterous Manipulation with Deep Reinforcement Learning and Demonstrations},
  author={Rajeswaran, Aravind and Kumar, Vikash and Gupta, Abhishek and Vezzani, Giulia and Schulman, John and Todorov, Emanuel and Levine, Sergey},
  journal={Robotics: Science and Systems XIV},
  year={2018},
  publisher={Robotics: Science and Systems Foundation}
}

@article{qin2022from,
  title={From one hand to multiple hands: Imitation learning for dexterous manipulation from single-camera teleoperation},
  author={Qin, Yuzhe and Su, Hao and Wang, Xiaolong},
  journal={IEEE Robotics and Automation Letters},
  volume={7},
  number={4},
  pages={10873--10881},
  year={2022},
  publisher={IEEE}
}

@inproceedings{qin2021dexmv,
  title={Dexmv: Imitation learning for dexterous manipulation from human videos},
  author={Qin, Yuzhe and Wu, Yueh-Hua and Liu, Shaowei and Jiang, Hanwen and Yang, Ruihan and Fu, Yang and Wang, Xiaolong},
  booktitle={European Conference on Computer Vision},
  pages={570--587},
  year={2022},
  organization={Springer}
}

@article{shadowhand,
  title={Learning dexterous in-hand manipulation},
  author={Andrychowicz, OpenAI: Marcin and Baker, Bowen and Chociej, Maciek and Jozefowicz, Rafal and McGrew, Bob and Pachocki, Jakub and Petron, Arthur and Plappert, Matthias and Powell, Glenn and Ray, Alex and others},
  journal={The International Journal of Robotics Research},
  volume={39},
  number={1},
  pages={3--20},
  year={2020},
  publisher={SAGE Publications Sage UK: London, England}
}

@inproceedings{ho2020denoising,
 author = {Ho, Jonathan and Jain, Ajay and Abbeel, Pieter},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
 pages = {6840--6851},
 publisher = {Curran Associates, Inc.},
 title = {Denoising Diffusion Probabilistic Models},
 url = {https://proceedings.neurips.cc/paper_files/paper/2020/file/4c5bcfec8584af0d967f1ab10179ca4b-Paper.pdf},
 volume = {33},
 year = {2020}
}

% pink noise, C8
@inproceedings{eberhard2023pink,
  title={Pink noise is all you need: Colored noise exploration in deep reinforcement learning},
  author={Eberhard, Onno and Hollenstein, Jakob and Pinneri, Cristina and Martius, Georg},
  booktitle={The Eleventh International Conference on Learning Representations},
  year={2023}
}

@article{Biyik2022Learning,
    author = {Erdem Bıyık and Dylan P. Losey and Malayandi Palan and Nicholas C. Landolfi and Gleb Shevchuk and Dorsa Sadigh},
    title = {Learning reward functions from diverse sources of human feedback: Optimally integrating demonstrations and preferences},
    journal = {The International Journal of Robotics Research},
    volume = {41},
    number = {1},
    pages = {45-67},
    year = {2022},
    doi = {10.1177/02783649211041652},
    URL = {https://doi.org/10.1177/02783649211041652}
}

@inproceedings{
	narang2022factory,
	author = {Yashraj Narang and Kier Storey and Iretiayo Akinola and Miles Macklin and Philipp Reist and Lukasz Wawrzyniak and Yunrong Guo and Adam Moravanszky and Gavriel State and Michelle Lu and Ankur Handa and Dieter Fox},
	title = {Factory: Fast contact for robotic assembly},
	booktitle = {Robotics: Science and Systems},
	year = {2022}
} 